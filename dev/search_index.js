var documenterSearchIndex = {"docs":
[{"location":"examples/surrogates/#Wing-Weight-Surrogate-Example","page":"Wing Weight Surrogate","title":"Wing Weight Surrogate Example","text":"This example demonstrates GP surrogate modeling using the aircraft wing weight simulator from Chapter 1 of \"Surrogates\" by Robert Gramacy.","category":"section"},{"location":"examples/surrogates/#Overview","page":"Wing Weight Surrogate","title":"Overview","text":"We will:\n\nBuild a GP surrogate for a 9-dimensional wing weight function\nCompare isotropic vs separable (ARD) GP models\nPerform main effects/sensitivity analysis\nVisualize predictions on 2D slices","category":"section"},{"location":"examples/surrogates/#Setup","page":"Wing Weight Surrogate","title":"Setup","text":"using laGP\nusing Random\nusing LatinHypercubeSampling\nusing Statistics: mean\n\nRandom.seed!(42)","category":"section"},{"location":"examples/surrogates/#Wing-Weight-Simulator","page":"Wing Weight Surrogate","title":"Wing Weight Simulator","text":"The simulator computes aircraft wing structural weight based on 9 design parameters:\n\nfunction wingwt(; Sw=0.48, Wfw=0.4, A=0.38, L=0.5, q=0.62,\n                  l=0.344, Rtc=0.4, Nz=0.37, Wdg=0.38)\n    # Transform coded [0,1] inputs to natural units\n    Sw_nat = Sw * (200 - 150) + 150       # Wing area (ft²)\n    Wfw_nat = Wfw * (300 - 220) + 220     # Fuel weight (lb)\n    A_nat = A * (10 - 6) + 6              # Aspect ratio\n    L_nat = (L * 20 - 10) * π / 180       # Sweep angle (rad)\n    q_nat = q * (45 - 16) + 16            # Dynamic pressure (lb/ft²)\n    l_nat = l * 0.5 + 0.5                 # Taper ratio\n    Rtc_nat = Rtc * 0.1 + 0.08            # Thickness ratio\n    Nz_nat = Nz * 3.5 + 2.5               # Load factor\n    Wdg_nat = Wdg * 800 + 1700            # Gross weight (lb)\n\n    # Wing weight formula\n    W = 0.036 * Sw_nat^0.758 * Wfw_nat^0.0035\n    W *= (A_nat / cos(L_nat)^2)^0.6\n    W *= q_nat^0.006 * l_nat^0.04\n    W *= (100 * Rtc_nat / cos(L_nat))^(-0.3)\n    W *= (Nz_nat * Wdg_nat)^0.49\n\n    return W\nend\n\n# Variable names\nvar_names = [\"Sw\", \"Wfw\", \"A\", \"L\", \"q\", \"l\", \"Rtc\", \"Nz\", \"Wdg\"]\n\n# Cessna C172 baseline\nbaseline = [0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38]","category":"section"},{"location":"examples/surrogates/#Generate-Training-Data","page":"Wing Weight Surrogate","title":"Generate Training Data","text":"Use Latin Hypercube Sampling for space-filling design:\n\nn = 1000\nplan, _ = LHCoptim(n, 9, 10)\nX = Matrix{Float64}(plan ./ n)\n\n# Evaluate simulator\nY = [wingwt(Sw=X[i,1], Wfw=X[i,2], A=X[i,3], L=X[i,4],\n            q=X[i,5], l=X[i,6], Rtc=X[i,7], Nz=X[i,8], Wdg=X[i,9])\n     for i in 1:n]\n\nprintln(\"Training data: $n points in 9 dimensions\")\nprintln(\"Response range: [$(round(minimum(Y), digits=2)), $(round(maximum(Y), digits=2))] lb\")\n\nExample response surfaces from Chapter 1:\n\n(Image: Banana Yield Function)\n\n(Image: RSM Function Gallery)\n\nThe Latin Hypercube design provides space-filling coverage:\n\n(Image: LHS Design Projection)","category":"section"},{"location":"examples/surrogates/#Fit-Isotropic-GP","page":"Wing Weight Surrogate","title":"Fit Isotropic GP","text":"# Get hyperparameter ranges\nd_range = darg(X)\ng_range = garg(Y)\n\n# Fit isotropic GP\ngp_iso = new_gp(X, Y, d_range.start, g_range.start)\njmle_gp(gp_iso; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Isotropic GP:\")\nprintln(\"  d = $(round(gp_iso.d, sigdigits=4))\")\nprintln(\"  g = $(round(gp_iso.g, sigdigits=4))\")\nprintln(\"  log-likelihood = $(round(llik_gp(gp_iso), digits=2))\")","category":"section"},{"location":"examples/surrogates/#Fit-Separable-GP","page":"Wing Weight Surrogate","title":"Fit Separable GP","text":"# Per-dimension lengthscale ranges\nd_range_sep = darg_sep(X)\n\nd_start_sep = [r.start for r in d_range_sep.ranges]\nd_ranges_sep = [(r.min, r.max) for r in d_range_sep.ranges]\n\n# Fit separable GP\ngp_sep = new_gp_sep(X, Y, d_start_sep, g_range.start)\njmle_gp_sep(gp_sep; drange=d_ranges_sep, grange=(g_range.min, g_range.max))\n\nprintln(\"Separable GP:\")\nprintln(\"  Lengthscales:\")\nfor (j, name) in enumerate(var_names)\n    println(\"    $name: $(round(gp_sep.d[j], sigdigits=4))\")\nend\nprintln(\"  g = $(round(gp_sep.g, sigdigits=4))\")\nprintln(\"  log-likelihood = $(round(llik_gp_sep(gp_sep), digits=2))\")","category":"section"},{"location":"examples/surrogates/#Compare-on-2D-Slice","page":"Wing Weight Surrogate","title":"Compare on 2D Slice","text":"Predict on A (aspect ratio) × Nz (load factor) slice:\n\nn_pred = 100\nx_pred = range(0.0, 1.0, length=n_pred)\n\n# Create prediction grid (vary A and Nz, fix others at baseline)\nXX = Matrix{Float64}(undef, n_pred^2, 9)\nfor i in 1:n_pred^2\n    XX[i, :] .= baseline\nend\n\nidx = 1\nfor nz in x_pred\n    for a in x_pred\n        XX[idx, 3] = a   # A\n        XX[idx, 8] = nz  # Nz\n        idx += 1\n    end\nend\n\n# Predictions\npred_iso = pred_gp(gp_iso, XX; lite=true)\npred_sep = pred_gp_sep(gp_sep, XX; lite=true)\n\n# True values for comparison\ntrue_vals = [wingwt(Sw=baseline[1], Wfw=baseline[2], A=a, L=baseline[4],\n                    q=baseline[5], l=baseline[6], Rtc=baseline[7], Nz=nz, Wdg=baseline[9])\n             for a in x_pred, nz in x_pred]\n\n# RMSE\nrmse_iso = sqrt(mean((vec(true_vals) .- pred_iso.mean).^2))\nrmse_sep = sqrt(mean((vec(true_vals) .- pred_sep.mean).^2))\n\nprintln(\"A × Nz slice RMSE:\")\nprintln(\"  Isotropic: $(round(rmse_iso, digits=4)) lb\")\nprintln(\"  Separable: $(round(rmse_sep, digits=4)) lb\")\n\n2D slice visualizations of the wing weight function:\n\n(Image: Wing Weight A×Nz Slice)\n\n(Image: Wing Weight λ×Wfw Slice)\n\nIsotropic vs Separable GP comparison on the A×Nz slice:\n\n(Image: Surrogate Comparison (A×Nz))\n\n(Image: Surrogate Comparison (λ×Wfw))","category":"section"},{"location":"examples/surrogates/#Main-Effects-Analysis","page":"Wing Weight Surrogate","title":"Main Effects Analysis","text":"Compute sensitivity of each input while holding others at baseline:\n\nn_me = 100\nx_me = range(0.0, 1.0, length=n_me)\n\nmain_effects = Matrix{Float64}(undef, n_me, 9)\n\nfor j in 1:9\n    # Create prediction matrix\n    XX_me = repeat(baseline', n_me, 1)\n    XX_me[:, j] = collect(x_me)\n\n    # Predict using separable GP\n    pred_me = pred_gp_sep(gp_sep, XX_me; lite=true)\n    main_effects[:, j] = pred_me.mean\nend\n\n# Report sensitivity (range of main effect)\nprintln(\"Variable sensitivity (range of main effect):\")\nfor j in 1:9\n    effect_range = maximum(main_effects[:, j]) - minimum(main_effects[:, j])\n    println(\"  $(var_names[j]): $(round(effect_range, digits=2)) lb\")\nend","category":"section"},{"location":"examples/surrogates/#Visualization-(with-CairoMakie)","page":"Wing Weight Surrogate","title":"Visualization (with CairoMakie)","text":"using CairoMakie\n\n# Main effects plot\nfig = Figure(size=(800, 500))\nax = Axis(fig[1, 1],\n    xlabel=\"Coded Input [0, 1]\",\n    ylabel=\"Wing Weight (lb)\",\n    title=\"Main Effects Analysis\"\n)\n\ncolors = [:blue, :red, :green, :orange, :purple, :brown, :pink, :gray, :cyan]\n\nfor j in 1:9\n    lines!(ax, collect(x_me), main_effects[:, j],\n           color=colors[j], linewidth=2, label=var_names[j])\nend\n\nLegend(fig[1, 2], ax, nbanks=1)\nfig\n\nThe main effects analysis reveals input sensitivities:\n\n(Image: Main Effects Analysis)","category":"section"},{"location":"examples/surrogates/#Key-Findings","page":"Wing Weight Surrogate","title":"Key Findings","text":"Most influential inputs: Sw (wing area), A (aspect ratio), Nz (load factor)\nLeast influential inputs: l (taper ratio), Wfw (fuel weight)\nIsotropic vs Separable: For smooth functions, both may converge to similar predictions\nLengthscale interpretation: Smaller lengthscale = more sensitive input","category":"section"},{"location":"examples/surrogates/#Surrogate-Benefits","page":"Wing Weight Surrogate","title":"Surrogate Benefits","text":"Speed: Surrogate predictions are instantaneous vs expensive simulations\nOptimization: Use GP for derivative-free optimization\nUncertainty: Predictive variance quantifies surrogate accuracy\nSensitivity: Lengthscales directly indicate input importance","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#Core-GP-Functions-(Isotropic)","page":"API Reference","title":"Core GP Functions (Isotropic)","text":"","category":"section"},{"location":"api/#Core-GP-Functions-(Separable)","page":"API Reference","title":"Core GP Functions (Separable)","text":"","category":"section"},{"location":"api/#MLE-Functions-(Isotropic)","page":"API Reference","title":"MLE Functions (Isotropic)","text":"","category":"section"},{"location":"api/#MLE-Functions-(Separable)","page":"API Reference","title":"MLE Functions (Separable)","text":"","category":"section"},{"location":"api/#Acquisition-Functions","page":"API Reference","title":"Acquisition Functions","text":"","category":"section"},{"location":"api/#Local-GP-Functions","page":"API Reference","title":"Local GP Functions","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#laGP.GP","page":"API Reference","title":"laGP.GP","text":"GP{T<:Real, K}\n\nGaussian Process model backed by AbstractGPs.jl with isotropic squared-exponential kernel.\n\nThis type uses AbstractGPs for the posterior computation while preserving laGP-specific quantities needed for the concentrated likelihood formula.\n\nFields\n\nX::Matrix{T}: n x m design matrix (n observations, m dimensions)\nZ::Vector{T}: n response values\nkernel::K: Kernel from KernelFunctions.jl\nchol::Cholesky{T}: Cholesky factorization of K + g*I\nKiZ::Vector{T}: K \\ Z (precomputed for prediction)\nd::T: lengthscale parameter (laGP parameterization)\ng::T: nugget parameter\nphi::T: Z' * Ki * Z (used for variance scaling)\nldetK::T: log determinant of K (used for likelihood)\n\nNotes\n\nThe AbstractGPs posterior can be reconstructed from (X, Z, kernel, g) when needed. We cache the Cholesky and derived quantities for efficient repeated computations.\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.GPsep","page":"API Reference","title":"laGP.GPsep","text":"GPsep{T<:Real, K}\n\nSeparable Gaussian Process model backed by AbstractGPs.jl with anisotropic kernel.\n\nUses a vector of lengthscales (one per input dimension) to capture varying input sensitivities.\n\nFields\n\nX::Matrix{T}: n x m design matrix (n observations, m dimensions)\nZ::Vector{T}: n response values\nkernel::K: ARD kernel from KernelFunctions.jl\nchol::Cholesky{T}: Cholesky factorization of K + g*I\nKiZ::Vector{T}: K \\ Z (precomputed for prediction)\nd::Vector{T}: lengthscale parameters (m elements, one per dimension)\ng::T: nugget parameter\nphi::T: Z' * Ki * Z (used for variance scaling)\nldetK::T: log determinant of K (used for likelihood)\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.GPPrediction","page":"API Reference","title":"laGP.GPPrediction","text":"GPPrediction{T<:Real}\n\nResult of GP prediction.\n\nFields\n\nmean::Vector{T}: predicted mean values\ns2::Vector{T}: predicted variances (if lite=true) or full covariance\ndf::Int: degrees of freedom (n observations)\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.GPPredictionFull","page":"API Reference","title":"laGP.GPPredictionFull","text":"GPPredictionFull{T<:Real}\n\nResult of GP prediction with full covariance matrix.\n\nFields\n\nmean::Vector{T}: predicted mean values\nSigma::Matrix{T}: full posterior covariance matrix (ntest x ntest)\ndf::Int: degrees of freedom (n observations)\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.new_gp","page":"API Reference","title":"laGP.new_gp","text":"new_gp(X, Z, d, g)\n\nCreate a new Gaussian Process model using AbstractGPs.jl backend.\n\nArguments\n\nX::Matrix: n x m design matrix (n observations, m dimensions)\nZ::Vector: n response values\nd::Real: lengthscale parameter (laGP parameterization)\ng::Real: nugget parameter\n\nReturns\n\nGP: Gaussian Process model backed by AbstractGPs\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.pred_gp","page":"API Reference","title":"laGP.pred_gp","text":"pred_gp(gp, XX; lite=true)\n\nMake predictions at test locations XX using AbstractGPs-backed GP.\n\nArguments\n\ngp::GP: Gaussian Process model\nXX::Matrix: test locations (n_test x m)\nlite::Bool: if true, return only diagonal variances\n\nReturns\n\nGPPrediction: prediction results with mean, s2, and df\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.llik_gp","page":"API Reference","title":"laGP.llik_gp","text":"llik_gp(gp)\n\nCompute the log-likelihood of the GP.\n\nUses the concentrated likelihood formula from R laGP:     llik = -0.5 * (n * log(0.5 * phi) + ldetK)\n\nArguments\n\ngp::GP: Gaussian Process model\n\nReturns\n\nReal: log-likelihood value\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.dllik_gp","page":"API Reference","title":"laGP.dllik_gp","text":"dllik_gp(gp; dg=true, dd=true)\n\nCompute gradient of log-likelihood w.r.t. d (lengthscale) and g (nugget).\n\nArguments\n\ngp::GP: Gaussian Process model\ndg::Bool: compute gradient w.r.t. nugget g (default: true)\ndd::Bool: compute gradient w.r.t. lengthscale d (default: true)\n\nReturns\n\nNamedTuple: (dllg=..., dlld=...) gradients\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.update_gp!","page":"API Reference","title":"laGP.update_gp!","text":"update_gp!(gp; d=nothing, g=nothing)\n\nUpdate GP hyperparameters and recompute internal quantities.\n\nArguments\n\ngp::GP: Gaussian Process model\nd::Real: new lengthscale (optional)\ng::Real: new nugget (optional)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.new_gp_sep","page":"API Reference","title":"laGP.new_gp_sep","text":"new_gp_sep(X, Z, d, g)\n\nCreate a new separable Gaussian Process model using AbstractGPs.jl backend.\n\nArguments\n\nX::Matrix: n x m design matrix (n observations, m dimensions)\nZ::Vector: n response values\nd::Vector: lengthscale parameters (m elements, one per dimension)\ng::Real: nugget parameter\n\nReturns\n\nGPsep: Separable Gaussian Process model backed by AbstractGPs\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.pred_gp_sep","page":"API Reference","title":"laGP.pred_gp_sep","text":"pred_gp_sep(gp, XX; lite=true)\n\nMake predictions at test locations XX using AbstractGPs-backed separable GP.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\nXX::Matrix: test locations (n_test x m)\nlite::Bool: if true, return only diagonal variances\n\nReturns\n\nGPPrediction: prediction results with mean, s2, and df\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.llik_gp_sep","page":"API Reference","title":"laGP.llik_gp_sep","text":"llik_gp_sep(gp)\n\nCompute the log-likelihood of the GPsep.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\n\nReturns\n\nReal: log-likelihood value\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.dllik_gp_sep","page":"API Reference","title":"laGP.dllik_gp_sep","text":"dllik_gp_sep(gp; dg=true, dd=true)\n\nCompute gradient of log-likelihood w.r.t. d (lengthscales) and g (nugget).\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\ndg::Bool: compute gradient w.r.t. nugget g (default: true)\ndd::Bool: compute gradient w.r.t. lengthscales d (default: true)\n\nReturns\n\nNamedTuple: (dllg=..., dlld=...) gradients\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.update_gp_sep!","page":"API Reference","title":"laGP.update_gp_sep!","text":"update_gp_sep!(gp; d=nothing, g=nothing)\n\nUpdate GPsep hyperparameters and recompute internal quantities.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\nd::Vector{Real}: new lengthscales (optional)\ng::Real: new nugget (optional)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.mle_gp","page":"API Reference","title":"laGP.mle_gp","text":"mle_gp(gp, param; tmax, tmin=sqrt(eps(T)))\n\nOptimize a single GP hyperparameter via maximum likelihood.\n\nArguments\n\ngp::GP: Gaussian Process model (modified in-place)\nparam::Symbol: parameter to optimize (:d or :g)\ntmax::Real: maximum value for parameter (required)\ntmin::Real: minimum value for parameter (default: sqrt(eps(T)), matching R's behavior)\n\nReturns\n\nNamedTuple: (d=..., g=..., its=..., msg=...) optimization result\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.jmle_gp","page":"API Reference","title":"laGP.jmle_gp","text":"jmle_gp(gp; drange, grange, maxit=100, verb=0, dab=(3/2, nothing), gab=(3/2, nothing))\n\nJoint MLE optimization of d and g for GP.\n\nArguments\n\ngp::GP: Gaussian Process model (modified in-place)\ndrange::Tuple: (min, max) range for d\ngrange::Tuple: (min, max) range for g\nmaxit::Int: maximum iterations\nverb::Int: verbosity level\ndab::Tuple: (shape, scale) for d prior\ngab::Tuple: (shape, scale) for g prior\n\nReturns\n\nNamedTuple: (d=..., g=..., tot_its=..., msg=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.darg","page":"API Reference","title":"laGP.darg","text":"darg(X; d=nothing, ab=(3/2, nothing))\n\nCompute default arguments for lengthscale parameter.\n\nBased on pairwise distances in the design matrix X.\n\nArguments\n\nX::Matrix: design matrix\nd::Union{Nothing,Real}: user-specified d (optional)\nab::Tuple: (shape, scale) for Inverse-Gamma prior; if scale=nothing, computed from range\n\nReturns\n\nNamedTuple: (start=..., min=..., max=..., mle=..., ab=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.garg","page":"API Reference","title":"laGP.garg","text":"garg(Z; g=nothing, ab=(3/2, nothing))\n\nCompute default arguments for nugget parameter.\n\nBased on squared residuals from the mean.\n\nArguments\n\nZ::Vector: response values\ng::Union{Nothing,Real}: user-specified g (optional)\nab::Tuple: (shape, scale) for Inverse-Gamma prior; if scale=nothing, computed from range\n\nReturns\n\nNamedTuple: (start=..., min=..., max=..., mle=..., ab=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.mle_gp_sep","page":"API Reference","title":"laGP.mle_gp_sep","text":"mle_gp_sep(gp, param, dim; tmax, tmin=sqrt(eps(T)))\n\nOptimize a single hyperparameter of a separable GP via maximum likelihood.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model (modified in-place)\nparam::Symbol: parameter to optimize (:d or :g)\ndim::Int: dimension index for :d (ignored for :g)\ntmax::Real: maximum value for parameter (required)\ntmin::Real: minimum value for parameter (default: sqrt(eps(T)), matching R's behavior)\n\nReturns\n\nNamedTuple: (d=..., g=..., its=..., msg=...) optimization result\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.jmle_gp_sep","page":"API Reference","title":"laGP.jmle_gp_sep","text":"jmle_gp_sep(gp; drange, grange, maxit=100, verb=0, dab=(3/2, nothing), gab=(3/2, nothing))\n\nJoint MLE optimization of lengthscales and nugget for GPsep.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model (modified in-place)\ndrange::Union{Tuple,Vector}: range for d parameters\ngrange::Tuple: (min, max) range for g\nmaxit::Int: maximum iterations\nverb::Int: verbosity level\ndab::Tuple: (shape, scale) for d prior\ngab::Tuple: (shape, scale) for g prior\n\nReturns\n\nNamedTuple: (d=..., g=..., tot_its=..., msg=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.darg_sep","page":"API Reference","title":"laGP.darg_sep","text":"darg_sep(X; d=nothing, ab=(3/2, nothing))\n\nCompute default arguments for lengthscale parameters (separable version).\n\nFollowing R's laGP convention, uses the TOTAL pairwise distance to compute initial ranges (same for all dimensions), then MLE finds per-dimension scaling.\n\nArguments\n\nX::Matrix: design matrix\nd::Union{Nothing,Vector}: user-specified d (optional)\nab::Tuple: (shape, scale) for Inverse-Gamma prior; if scale=nothing, computed from range\n\nReturns\n\nNamedTuple: (ranges=..., ab=...) where ranges is Vector of per-dimension NamedTuples\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.alc_gp","page":"API Reference","title":"laGP.alc_gp","text":"alc_gp(gp, Xcand, Xref)\n\nCompute Active Learning Cohn (ALC) acquisition values.\n\nALC measures expected variance reduction at reference points Xref if we were to add each candidate point from Xcand to the design.\n\nArguments\n\ngp::GP: Gaussian Process model\nXcand::Matrix: candidate points (n_cand x m)\nXref::Matrix: reference points (n_ref x m)\n\nReturns\n\nVector: ALC values for each candidate point\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.mspe_gp","page":"API Reference","title":"laGP.mspe_gp","text":"mspe_gp(gp, Xcand, Xref)\n\nCompute Mean Squared Prediction Error (MSPE) acquisition values.\n\nMSPE is related to ALC and includes the current prediction variance.\n\nArguments\n\ngp::GP: Gaussian Process model\nXcand::Matrix: candidate points (n_cand x m)\nXref::Matrix: reference points (n_ref x m)\n\nReturns\n\nVector: MSPE values for each candidate point\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.lagp","page":"API Reference","title":"laGP.lagp","text":"lagp(Xref, start, endpt, X, Z; d, g, method=:alc, verb=0)\n\nLocal Approximate GP prediction at a single reference point.\n\nBuilds a local GP by starting with nearest neighbors and sequentially adding points that maximize the chosen acquisition function.\n\nArguments\n\nXref::Vector: single reference point (length m)\nstart::Int: initial number of nearest neighbors\nendpt::Int: final local design size\nX::Matrix: full training design (n x m)\nZ::Vector: full training responses\nd::Real: lengthscale parameter\ng::Real: nugget parameter\nmethod::Symbol: acquisition method (:alc, :mspe, or :nn)\nverb::Int: verbosity level\n\nReturns\n\nNamedTuple: (mean=..., var=..., df=..., indices=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.agp","page":"API Reference","title":"laGP.agp","text":"agp(X, Z, XX; start=6, endpt=50, d, g, method=:alc, verb=0, parallel=true)\n\nApproximate GP predictions at multiple reference points.\n\nCalls lagp for each row of XX, optionally in parallel using threads.\n\nArguments\n\nX::Matrix: training design (n x m)\nZ::Vector: training responses\nXX::Matrix: test/reference points (n_test x m)\nstart::Int: initial number of nearest neighbors\nendpt::Int: final local design size\nd::Union{Real,NamedTuple}: lengthscale parameter or (start, mle, min, max)\ng::Union{Real,NamedTuple}: nugget parameter or (start, mle, min, max)\nmethod::Symbol: acquisition method (:alc, :mspe, or :nn)\nverb::Int: verbosity level\nparallel::Bool: use multi-threading\n\nReturns\n\nNamedTuple: (mean=..., var=..., df=..., mle=...)\n\n\n\n\n\n","category":"function"},{"location":"examples/motorcycle/#Motorcycle-Crash-Test-Example","page":"Motorcycle Crash Test","title":"Motorcycle Crash Test Example","text":"This example demonstrates fitting both a full GP and local approximate GP (aGP) to the classic motorcycle crash test dataset, reproducing results from the R laGP package.","category":"section"},{"location":"examples/motorcycle/#Overview","page":"Motorcycle Crash Test","title":"Overview","text":"We will:\n\nFit a full GP to the mcycle dataset (133 observations)\nCompare with local approximate GP (aGP) predictions\nScale to a larger dataset using fixed hyperparameters\n\nThe mcycle dataset contains head acceleration (g) measurements at various times (ms) after a simulated motorcycle crash.","category":"section"},{"location":"examples/motorcycle/#Setup","page":"Motorcycle Crash Test","title":"Setup","text":"using laGP\nusing Distributions\nusing Random\nusing CairoMakie","category":"section"},{"location":"examples/motorcycle/#Dataset","page":"Motorcycle Crash Test","title":"Dataset","text":"The mcycle dataset from the MASS R package contains 133 observations:\n\n# Embedded mcycle dataset (times in ms, acceleration in g)\nX = reshape(mcycle_times, :, 1)  # 133 x 1 design matrix\nZ = mcycle_accel                  # 133 response values\n\nprintln(\"Motorcycle Crash Test Data\")\nprintln(\"  Observations: \", length(Z))\nprintln(\"  Time range: \", minimum(X), \" - \", maximum(X), \" ms\")\nprintln(\"  Accel range: \", minimum(Z), \" - \", maximum(Z), \" g\")","category":"section"},{"location":"examples/motorcycle/#Data-Driven-Parameter-Initialization","page":"Motorcycle Crash Test","title":"Data-Driven Parameter Initialization","text":"Use darg and garg to get sensible parameter ranges:\n\n# Get data-driven parameter ranges\nda = darg(X)\nga = garg(Z)\n\nprintln(\"Data-driven parameter ranges:\")\nprintln(\"  d: start=\", round(da.start, digits=4),\n        \", min=\", round(da.min, digits=4),\n        \", max=\", round(da.max, digits=4))\nprintln(\"  g: start=\", round(ga.start, digits=4),\n        \", min=\", round(ga.min, digits=6),\n        \", max=\", round(ga.max, digits=4))","category":"section"},{"location":"examples/motorcycle/#Full-GP-with-Joint-MLE","page":"Motorcycle Crash Test","title":"Full GP with Joint MLE","text":"Fit a full GP and optimize both lengthscale (d) and nugget (g):\n\n# Create GP with data-driven initial values\ngp = new_gp(X, Z, da.start, ga.start)\n\n# Joint MLE for both d and g\nresult = jmle_gp(gp; drange=(da.min, da.max), grange=(ga.min, ga.max))\n\nprintln(\"MLE results:\")\nprintln(\"  d = \", round(gp.d, digits=4))\nprintln(\"  g = \", round(gp.g, digits=4))\nprintln(\"  iterations = \", result.tot_its)","category":"section"},{"location":"examples/motorcycle/#Local-Approximate-GP-(aGP)","page":"Motorcycle Crash Test","title":"Local Approximate GP (aGP)","text":"The agp function builds local GP models at each prediction point using nearest neighbors:\n\n# Prediction grid\nxx = collect(range(minimum(X) - 2, maximum(X) + 2, length=200))\nXX = reshape(xx, :, 1)\n\n# Run aGP with MLE for both d and g\n# Use NamedTuples to enable MLE optimization at each prediction point\nd_agp = (start=da.start, min=da.min, max=da.max, mle=true)\ng_agp = (start=ga.start, min=ga.min, max=ga.max, mle=true)\n\npred_agp = agp(X, Z, XX; endpt=30, d=d_agp, g=g_agp, method=:alc, verb=0)\n\nprintln(\"aGP prediction complete\")\nprintln(\"  Test points: \", length(xx))\nprintln(\"  Local neighborhood size: 30\")","category":"section"},{"location":"examples/motorcycle/#Comparison:-Full-GP-vs-aGP","page":"Motorcycle Crash Test","title":"Comparison: Full GP vs aGP","text":"# Full GP predictions\npred_full_gp = pred_gp(gp, XX; lite=true)\n\n# 90% credible intervals\nz_90 = quantile(Normal(), 0.95)\n\n# Full GP intervals\ngp_lower = pred_full_gp.mean .- z_90 .* sqrt.(pred_full_gp.s2)\ngp_upper = pred_full_gp.mean .+ z_90 .* sqrt.(pred_full_gp.s2)\n\n# aGP intervals\nagp_lower = pred_agp.mean .- z_90 .* sqrt.(pred_agp.var)\nagp_upper = pred_agp.mean .+ z_90 .* sqrt.(pred_agp.var)","category":"section"},{"location":"examples/motorcycle/#Visualization","page":"Motorcycle Crash Test","title":"Visualization","text":"fig = Figure(size=(700, 500))\n\nax = Axis(fig[1, 1],\n          xlabel=\"Time (ms)\",\n          ylabel=\"Acceleration (g)\",\n          title=\"GP vs aGP Comparison\")\n\n# Data points\nscatter!(ax, vec(X), Z, color=:black, markersize=6)\n\n# Full GP: black solid mean, black dashed CIs\nlines!(ax, xx, pred_full_gp.mean, color=:black, linewidth=2, label=\"GP mean\")\nlines!(ax, xx, gp_lower, color=:black, linewidth=1, linestyle=:dash, label=\"GP 90% CI\")\nlines!(ax, xx, gp_upper, color=:black, linewidth=1, linestyle=:dash)\n\n# aGP: red solid mean, red dashed CIs\nlines!(ax, xx, pred_agp.mean, color=:red, linewidth=2, label=\"aGP mean\")\nlines!(ax, xx, agp_lower, color=:red, linewidth=1, linestyle=:dash, label=\"aGP 90% CI\")\nlines!(ax, xx, agp_upper, color=:red, linewidth=1, linestyle=:dash)\n\naxislegend(ax, position=:rb)\n\nfig\n\n(Image: GP vs aGP Comparison)\n\nThe full GP (black) and aGP (red) predictions are nearly identical for this dataset size. The aGP approach becomes advantageous for larger datasets where full GP computation is prohibitive.","category":"section"},{"location":"examples/motorcycle/#Scaling-to-Larger-Datasets","page":"Motorcycle Crash Test","title":"Scaling to Larger Datasets","text":"When scaling to larger datasets, use fixed hyperparameters from the original MLE rather than re-optimizing. This matches the R laGP approach and prevents overfitting:\n\n# Replicate data 10 times with jitter on X\nRandom.seed!(42)\nn_rep = 10\nX_big = repeat(X, n_rep) .+ randn(n * n_rep) .* 1.0  # Jitter on X (sd=1)\nZ_big = repeat(Z, n_rep)                              # No noise on Z\n\nprintln(\"Enlarged dataset:\")\nprintln(\"  Observations: \", length(Z_big))  # 1330 observations\n\n# aGP with FIXED hyperparameters from the full GP MLE\n# This matches R: aGP(X, Z, XX, end = 30, d = d, g = g, verb = 0)\npred_agp_big = agp(X_big, Z_big, XX; endpt=30, d=gp.d, g=gp.g,\n                   method=:alc, verb=0)","category":"section"},{"location":"examples/motorcycle/#Visualization-2","page":"Motorcycle Crash Test","title":"Visualization","text":"fig2 = Figure(size=(700, 500))\n\nax = Axis(fig2[1, 1],\n          xlabel=\"Time (ms)\",\n          ylabel=\"Acceleration (g)\",\n          title=\"aGP on Enlarged Motorcycle Data (n=1330)\")\n\nscatter!(ax, vec(X_big), Z_big, color=(:gray, 0.3), markersize=4,\n         label=\"Data (10x replicated)\")\nlines!(ax, xx, pred_agp_big.mean, color=:red, linewidth=2, label=\"aGP mean\")\n\n# 90% credible intervals\nagp_big_lower = pred_agp_big.mean .- z_90 .* sqrt.(pred_agp_big.var)\nagp_big_upper = pred_agp_big.mean .+ z_90 .* sqrt.(pred_agp_big.var)\nlines!(ax, xx, agp_big_lower, color=:red, linewidth=1, linestyle=:dash, label=\"aGP 90% CI\")\nlines!(ax, xx, agp_big_upper, color=:red, linewidth=1, linestyle=:dash)\n\naxislegend(ax, position=:rb)\n\nfig2\n\n(Image: aGP on Enlarged Dataset)","category":"section"},{"location":"examples/motorcycle/#Key-Concepts","page":"Motorcycle Crash Test","title":"Key Concepts","text":"","category":"section"},{"location":"examples/motorcycle/#Fixed-vs-Adaptive-Hyperparameters","page":"Motorcycle Crash Test","title":"Fixed vs Adaptive Hyperparameters","text":"When using agp with a NamedTuple like (start=..., min=..., max=..., mle=true), MLE optimization runs at each prediction point. This is appropriate for the original dataset.\n\nFor enlarged/replicated datasets, pass fixed scalar values (d=gp.d, g=gp.g) to use hyperparameters estimated from the original data. This:\n\nPrevents overfitting to artificial replication structure\nMatches the R laGP approach\nProduces smoother predictions","category":"section"},{"location":"examples/motorcycle/#aGP-Parameters","page":"Motorcycle Crash Test","title":"aGP Parameters","text":"endpt: Maximum local neighborhood size (default 50)\nmethod: Acquisition function for neighbor selection\n:alc - Active Learning Cohn (reduces predictive variance)\n:mspe - Mean Squared Prediction Error\n:nn - Simple nearest neighbors\nverb: Verbosity level (0 = silent)","category":"section"},{"location":"examples/motorcycle/#When-to-Use-aGP","page":"Motorcycle Crash Test","title":"When to Use aGP","text":"Large datasets: O(n) per prediction vs O(n³) for full GP\nNon-stationary functions: Local models adapt to local structure\nStreaming data: Can update local neighborhoods efficiently","category":"section"},{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"This page covers the mathematical background for laGP.jl.","category":"section"},{"location":"theory/#Gaussian-Process-Basics","page":"Theory","title":"Gaussian Process Basics","text":"A Gaussian Process (GP) is a collection of random variables, any finite subset of which have a joint Gaussian distribution. A GP is fully specified by its mean function m(x) and covariance function k(x x):\n\nf(x) sim mathcalGP(m(x) k(x x))\n\nGiven training data X Z where X is an n times m design matrix and Z is an n-vector of responses, the GP posterior at test points X_* is:\n\nbeginaligned\nmu_* = k(X_* X) K^-1 Z \nSigma_* = k(X_* X_*) - k(X_* X) K^-1 k(X X_*)\nendaligned\n\nwhere K = k(X X) + g I is the training covariance matrix with nugget g.","category":"section"},{"location":"theory/#Kernel-Parameterization","page":"Theory","title":"Kernel Parameterization","text":"","category":"section"},{"location":"theory/#Isotropic-Squared-Exponential","page":"Theory","title":"Isotropic Squared Exponential","text":"laGP uses the following parameterization:\n\nk(x y) = expleft(-fracx - y^2dright)\n\nwhere d is the lengthscale parameter.\n\nnote: KernelFunctions.jl Comparison\nKernelFunctions.jl uses: k(xy) = exp(-x-y^2(2ell^2))The mapping is: d = 2ell^2, so ell = sqrtd2","category":"section"},{"location":"theory/#Separable-(ARD)-Kernel","page":"Theory","title":"Separable (ARD) Kernel","text":"For separable/anisotropic GPs, each dimension has its own lengthscale:\n\nk(x y) = expleft(-sum_j=1^m frac(x_j - y_j)^2d_jright)\n\nThis allows the model to capture different smoothness in different input directions.","category":"section"},{"location":"theory/#Concentrated-Likelihood","page":"Theory","title":"Concentrated Likelihood","text":"laGP uses the concentrated (profile) log-likelihood:\n\nell = -frac12left(n logleft(fracphi2right) + logKright)\n\nwhere phi = Z^top K^-1 Z.\n\nThis formulation profiles out the variance parameter, leaving only d and g to optimize.","category":"section"},{"location":"theory/#Gradients","page":"Theory","title":"Gradients","text":"The gradient with respect to the nugget g:\n\nfracpartial ellpartial g = -frac12 texttr(K^-1) + fracn2phi (K^-1Z)^top (K^-1Z)\n\nThe gradient with respect to lengthscale d:\n\nfracpartial ellpartial d = -frac12 texttrleft(K^-1 fracpartial Kpartial dright) + fracn2phi Z^top K^-1 fracpartial Kpartial d K^-1 Z\n\nwhere for the isotropic kernel:\n\nfracpartial K_ijpartial d = K_ij fracx_i - x_j^2d^2","category":"section"},{"location":"theory/#Inverse-Gamma-Priors","page":"Theory","title":"Inverse-Gamma Priors","text":"MLE optimization uses Inverse-Gamma priors for regularization:\n\np(theta) propto theta^-a-1 expleft(-fracbthetaright)\n\nwhere a is the shape and b is the scale parameter.\n\nThe log-prior and its gradient:\n\nbeginaligned\nlog p(theta) = a log b - logGamma(a) - (a+1)logtheta - fracbtheta \nfracpartial log ppartial theta = -fraca+1theta + fracbtheta^2\nendaligned\n\nDefault values: a = 32, with b computed from the data-adaptive parameter ranges.","category":"section"},{"location":"theory/#Local-Approximate-GP","page":"Theory","title":"Local Approximate GP","text":"For large datasets, building a full GP is computationally prohibitive (O(n^3)). Local approximate GP builds a small local model for each prediction point.","category":"section"},{"location":"theory/#Algorithm","page":"Theory","title":"Algorithm","text":"For each prediction point x_*:\n\nInitialize: Select n_0 nearest neighbors to x_*\nIterate: Until local design has n_textend points:\nEvaluate acquisition function on all remaining candidates\nAdd the point that maximizes (ALC) or minimizes (MSPE) the criterion\nPredict: Make prediction using the local GP","category":"section"},{"location":"theory/#Active-Learning-Cohn-(ALC)","page":"Theory","title":"Active Learning Cohn (ALC)","text":"ALC measures the expected reduction in predictive variance at reference points if a candidate point were added:\n\ntextALC(x) = frac1n_textref sum_j=1^n_textref leftsigma^2(x_textrefj) - sigma^2_x(x_textrefj)right\n\nwhere sigma^2_x is the variance after adding point x to the design.\n\nHigher ALC values indicate points that would reduce prediction uncertainty more.","category":"section"},{"location":"theory/#Mean-Squared-Prediction-Error-(MSPE)","page":"Theory","title":"Mean Squared Prediction Error (MSPE)","text":"MSPE combines current variance with expected variance reduction:\n\ntextMSPE(x) = fracn+1n-1 barsigma^2 - fracn+1n(n-1) cdot fracn-2n cdot textALC(x)\n\nwhere barsigma^2 is the average predictive variance at reference points.\n\nLower MSPE values indicate better candidate points.","category":"section"},{"location":"theory/#Nearest-Neighbor-(NN)","page":"Theory","title":"Nearest Neighbor (NN)","text":"The simplest approach: just use the n_textend nearest neighbors. Fast but doesn't account for prediction location.","category":"section"},{"location":"theory/#Prediction-Variance-Scaling","page":"Theory","title":"Prediction Variance Scaling","text":"laGP scales the posterior variance using the Student-t distribution:\n\ns^2 = fracphin left(1 + g - k_*^top K^-1 k_*right)\n\nThis gives wider intervals than the standard GP formulation, accounting for hyperparameter uncertainty.\n\nThe degrees of freedom is n (number of training points).","category":"section"},{"location":"theory/#Data-Adaptive-Hyperparameter-Ranges","page":"Theory","title":"Data-Adaptive Hyperparameter Ranges","text":"","category":"section"},{"location":"theory/#Lengthscale-(darg)","page":"Theory","title":"Lengthscale (darg)","text":"The darg function computes ranges from pairwise distances:\n\nStart: 10th percentile of pairwise squared distances\nMin: Half the minimum non-zero distance (floored at sqrtepsilon)\nMax: Maximum pairwise distance","category":"section"},{"location":"theory/#Nugget-(garg)","page":"Theory","title":"Nugget (garg)","text":"The garg function computes ranges from response variability:\n\nStart: 2.5th percentile of squared residuals from mean\nMin: sqrtepsilon (machine epsilon)\nMax: Maximum squared residual","category":"section"},{"location":"theory/#References","page":"Theory","title":"References","text":"Gramacy, R. B. (2016). laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R. Journal of Statistical Software, 72(1), 1-46.\nGramacy, R. B. and Apley, D. W. (2015). Local Gaussian Process Approximation for Large Computer Experiments. Journal of Computational and Graphical Statistics, 24(2), 561-578.\nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This tutorial covers the basics of using laGP.jl for Gaussian Process regression.","category":"section"},{"location":"getting_started/#Creating-a-GP-Model","page":"Getting Started","title":"Creating a GP Model","text":"","category":"section"},{"location":"getting_started/#Isotropic-GP","page":"Getting Started","title":"Isotropic GP","text":"An isotropic GP uses a single lengthscale parameter for all dimensions:\n\nusing laGP\n\n# Training data: n observations, m dimensions\nX = rand(50, 2)  # 50 points in 2D\nZ = sin.(X[:, 1]) .+ cos.(X[:, 2]) + 0.1 * randn(50)\n\n# Initial hyperparameters\nd = 0.5  # lengthscale\ng = 0.01 # nugget (noise variance)\n\n# Create GP\ngp = new_gp(X, Z, d, g)","category":"section"},{"location":"getting_started/#Separable-GP-(ARD)","page":"Getting Started","title":"Separable GP (ARD)","text":"A separable GP uses per-dimension lengthscales:\n\n# Per-dimension lengthscales\nd = [0.5, 0.3]  # different lengthscale for each dimension\n\n# Create separable GP\ngp_sep = new_gp_sep(X, Z, d, g)","category":"section"},{"location":"getting_started/#Hyperparameter-Estimation","page":"Getting Started","title":"Hyperparameter Estimation","text":"","category":"section"},{"location":"getting_started/#Using-darg-and-garg","page":"Getting Started","title":"Using darg and garg","text":"The darg and garg functions compute sensible hyperparameter ranges from your data:\n\n# Get data-adaptive ranges\nd_range = darg(X)\ng_range = garg(Z)\n\nprintln(\"Lengthscale: start=$(d_range.start), range=[$(d_range.min), $(d_range.max)]\")\nprintln(\"Nugget: start=$(g_range.start), range=[$(g_range.min), $(g_range.max)]\")\n\n# Create GP with data-adaptive starting values\ngp = new_gp(X, Z, d_range.start, g_range.start)","category":"section"},{"location":"getting_started/#Maximum-Likelihood-Estimation","page":"Getting Started","title":"Maximum Likelihood Estimation","text":"Optimize hyperparameters using MLE:\n\n# Single parameter optimization\nmle_gp(gp, :d; tmax=d_range.max)  # optimize lengthscale only\n\n# Joint MLE of both parameters\njmle_gp(gp; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Optimized d: \", gp.d)\nprintln(\"Optimized g: \", gp.g)\n\nFor separable GPs:\n\nd_range_sep = darg_sep(X)\ngp_sep = new_gp_sep(X, Z, [r.start for r in d_range_sep.ranges], g_range.start)\n\njmle_gp_sep(gp_sep;\n    drange=[(r.min, r.max) for r in d_range_sep.ranges],\n    grange=(g_range.min, g_range.max)\n)\n\nprintln(\"Optimized lengthscales: \", gp_sep.d)","category":"section"},{"location":"getting_started/#Making-Predictions","page":"Getting Started","title":"Making Predictions","text":"","category":"section"},{"location":"getting_started/#Basic-Prediction","page":"Getting Started","title":"Basic Prediction","text":"# Test points\nX_test = rand(10, 2)\n\n# Predict (lite=true returns diagonal variances only)\npred = pred_gp(gp, X_test; lite=true)\n\nprintln(\"Mean: \", pred.mean)\nprintln(\"Variance: \", pred.s2)\nprintln(\"Degrees of freedom: \", pred.df)","category":"section"},{"location":"getting_started/#Full-Covariance-Matrix","page":"Getting Started","title":"Full Covariance Matrix","text":"For posterior sampling, get the full covariance:\n\n# Full prediction (lite=false returns full covariance matrix)\npred_full = pred_gp(gp, X_test; lite=false)\n\nprintln(\"Covariance matrix size: \", size(pred_full.Sigma))\n\n# Draw posterior samples\nusing Distributions\nmvn = MvNormal(pred_full.mean, Symmetric(pred_full.Sigma))\nsamples = rand(mvn, 100)  # 100 posterior samples","category":"section"},{"location":"getting_started/#Local-Approximate-GP","page":"Getting Started","title":"Local Approximate GP","text":"For large datasets, use local approximate GP:","category":"section"},{"location":"getting_started/#Single-Point-Prediction-with-lagp","page":"Getting Started","title":"Single Point Prediction with lagp","text":"# Large training set\nX_train = rand(5000, 2)\nZ_train = sin.(2π * X_train[:, 1]) .* cos.(2π * X_train[:, 2])\n\n# Reference point to predict\nXref = [0.5, 0.5]\n\n# Local GP prediction\nresult = lagp(Xref, 6, 50, X_train, Z_train;\n    d=0.1, g=1e-6,\n    method=:alc  # :alc, :mspe, or :nn\n)\n\nprintln(\"Prediction: \", result.mean)\nprintln(\"Variance: \", result.var)\nprintln(\"Local design indices: \", result.indices)","category":"section"},{"location":"getting_started/#Batch-Predictions-with-agp","page":"Getting Started","title":"Batch Predictions with agp","text":"# Multiple test points\nX_test = rand(100, 2)\n\n# Get hyperparameter ranges\nd_range = darg(X_train)\ng_range = garg(Z_train)\n\n# Approximate GP predictions\nresult = agp(X_train, Z_train, X_test;\n    start=6,   # initial nearest neighbors\n    endpt=50,  # final local design size\n    d=(start=d_range.start, mle=false),\n    g=(start=g_range.start, mle=false),\n    method=:alc,\n    parallel=true  # use multi-threading\n)\n\nprintln(\"Predictions shape: \", size(result.mean))","category":"section"},{"location":"getting_started/#Acquisition-Methods","page":"Getting Started","title":"Acquisition Methods","text":"laGP supports three acquisition methods for local design selection:\n\nMethod Description Use Case\n:alc Active Learning Cohn Best accuracy, slower\n:mspe Mean Squared Prediction Error Balance of speed/accuracy\n:nn Nearest Neighbors Fastest, less accurate\n\n# Compare methods\nresult_alc = agp(X_train, Z_train, X_test; method=:alc, ...)\nresult_mspe = agp(X_train, Z_train, X_test; method=:mspe, ...)\nresult_nn = agp(X_train, Z_train, X_test; method=:nn, ...)","category":"section"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"Theory: Mathematical background on GP kernels and acquisition functions\nExamples: Complete worked examples\nAPI Reference: Full function documentation","category":"section"},{"location":"examples/multivariate/#Multivariate-Inputs-with-ARD","page":"Multivariate Inputs (ARD)","title":"Multivariate Inputs with ARD","text":"This example demonstrates how to fit a separable GP (also known as ARD - Automatic Relevance Determination) to multi-dimensional input data, allowing the model to learn different lengthscales for each input dimension.","category":"section"},{"location":"examples/multivariate/#Why-Per-Dimension-Lengthscales?","page":"Multivariate Inputs (ARD)","title":"Why Per-Dimension Lengthscales?","text":"In standard (isotropic) GPs, a single lengthscale parameter controls how quickly the function varies across all input dimensions. This assumes the function changes at the same rate regardless of direction - an isotropic assumption.\n\nHowever, real-world functions are often anisotropic: they vary more rapidly in some directions than others. Consider:\n\nA function of (temperature, pressure) where small temperature changes have large effects but pressure changes matter less\nA spatial process where north-south correlation differs from east-west\nAny problem where some inputs are more \"relevant\" than others\n\nThe separable GP (or ARD kernel) addresses this by assigning a separate lengthscale d[k] to each input dimension k. The kernel becomes:\n\nk(x, x') = exp(-Σₖ (xₖ - x'ₖ)² / dₖ)\n\nA smaller lengthscale means the function varies more rapidly in that dimension (high sensitivity), while a larger lengthscale means smoother variation (low sensitivity). After fitting, the relative magnitudes of the lengthscales reveal which inputs matter most - hence \"Automatic Relevance Determination.\"","category":"section"},{"location":"examples/multivariate/#Setup","page":"Multivariate Inputs (ARD)","title":"Setup","text":"using laGP\nusing Random\nusing CairoMakie","category":"section"},{"location":"examples/multivariate/#Generate-Training-Data","page":"Multivariate Inputs (ARD)","title":"Generate Training Data","text":"We use a 2D test function from the tinygp tutorial:\n\n# Target function: y = sin(x₁) * cos(x₂ + x₁) + noise\n# This has anisotropic structure - the sin(x₁) term creates faster variation in x₁\n\nRandom.seed!(48392)\n\nn = 100\nX = -5.0 .+ 10.0 .* rand(n, 2)  # Uniform on [-5, 5]²\n\nyerr = 0.1\ny = sin.(X[:, 1]) .* cos.(X[:, 2] .+ X[:, 1]) .+ yerr .* randn(n)\n\nprintln(\"Training data: $n points in 2D, noise σ = $yerr\")","category":"section"},{"location":"examples/multivariate/#Initialize-Hyperparameters","page":"Multivariate Inputs (ARD)","title":"Initialize Hyperparameters","text":"Use data-driven initialization for the lengthscales and nugget:\n\n# darg_sep returns ranges suitable for separable GP\nd_info = darg_sep(X)\ng_info = garg(y)\n\nprintln(\"Lengthscale range: [$(d_info.ranges[1].min), $(d_info.ranges[1].max)]\")\nprintln(\"Nugget range: [$(g_info.min), $(g_info.max)]\")\n\n# Initial lengthscales (same for both dimensions)\nd_init = [d_info.ranges[1].start, d_info.ranges[2].start]","category":"section"},{"location":"examples/multivariate/#Create-and-Fit-Separable-GP","page":"Multivariate Inputs (ARD)","title":"Create and Fit Separable GP","text":"# Create GP with per-dimension lengthscales\ngp = new_gp_sep(X, y, d_init, g_info.start)\n\nprintln(\"Initial: d = $(round.(gp.d, sigdigits=4)), g = $(round(gp.g, sigdigits=4))\")\n\n# Joint MLE optimization\ndrange = (d_info.ranges[1].min, d_info.ranges[1].max)\ngrange = (g_info.min, g_info.max)\n\nresult = jmle_gp_sep(gp; drange=drange, grange=grange, verb=0)\n\nprintln(\"Optimized: d = $(round.(gp.d, sigdigits=4)), g = $(round(gp.g, sigdigits=4))\")\nprintln(\"Log-likelihood: $(round(llik_gp_sep(gp), sigdigits=4))\")","category":"section"},{"location":"examples/multivariate/#Interpreting-the-Lengthscales","page":"Multivariate Inputs (ARD)","title":"Interpreting the Lengthscales","text":"The optimized lengthscales reveal the function's anisotropic structure:\n\nprintln(\"Lengthscale interpretation:\")\nprintln(\"  d[1] (x₁ dimension) = $(round(gp.d[1], sigdigits=4))\")\nprintln(\"  d[2] (x₂ dimension) = $(round(gp.d[2], sigdigits=4))\")\n\nif gp.d[1] < gp.d[2]\n    ratio = gp.d[2] / gp.d[1]\n    println(\"  → x₁ is $(round(ratio, sigdigits=2))x more sensitive (smaller lengthscale)\")\nend\n\nFor our test function sin(x₁) * cos(x₂ + x₁), we expect d[1] < d[2] because:\n\nThe sin(x₁) term creates oscillations primarily along the x₁ axis\nThe cos(x₂ + x₁) term couples both dimensions but adds smoother x₂ dependence\n\nThe MLE correctly identifies that x₁ requires a shorter lengthscale to capture the faster variation.","category":"section"},{"location":"examples/multivariate/#Prediction","page":"Multivariate Inputs (ARD)","title":"Prediction","text":"# Create prediction grid\nn_x1, n_x2 = 100, 50\nx1_grid = range(-5, 5, length=n_x1)\nx2_grid = range(-5, 5, length=n_x2)\n\n# Build test matrix\nXX = Matrix{Float64}(undef, n_x1 * n_x2, 2)\nlet idx = 1\n    for j in 1:n_x2\n        for i in 1:n_x1\n            XX[idx, 1] = x1_grid[i]\n            XX[idx, 2] = x2_grid[j]\n            idx += 1\n        end\n    end\nend\n\n# Get predictions\npred = pred_gp_sep(gp, XX; lite=true)\n\n# Reshape for plotting\nmean_grid = reshape(pred.mean, n_x1, n_x2)\nstd_grid = reshape(sqrt.(pred.s2), n_x1, n_x2)","category":"section"},{"location":"examples/multivariate/#Visualization","page":"Multivariate Inputs (ARD)","title":"Visualization","text":"fig = Figure(size=(1000, 450))\n\n# Predicted mean surface\nax1 = Axis(fig[1, 1], xlabel=\"x₁\", ylabel=\"x₂\", title=\"Predicted Mean\")\nhm1 = heatmap!(ax1, collect(x1_grid), collect(x2_grid), mean_grid', colormap=:viridis)\nscatter!(ax1, X[:, 1], X[:, 2], color=:white, markersize=5,\n         strokecolor=:black, strokewidth=0.5)\nColorbar(fig[1, 2], hm1, label=\"Mean\")\n\n# Prediction uncertainty\nax2 = Axis(fig[1, 3], xlabel=\"x₁\", ylabel=\"x₂\", title=\"Prediction Uncertainty (Std)\")\nhm2 = heatmap!(ax2, collect(x1_grid), collect(x2_grid), std_grid', colormap=:plasma)\nscatter!(ax2, X[:, 1], X[:, 2], color=:white, markersize=5,\n         strokecolor=:black, strokewidth=0.5)\nColorbar(fig[1, 4], hm2, label=\"Std\")\n\n# Title with optimized hyperparameters\nLabel(fig[0, :],\n      \"Separable GP (ARD): d₁=$(round(gp.d[1], sigdigits=3)), d₂=$(round(gp.d[2], sigdigits=3)), g=$(round(gp.g, sigdigits=3))\",\n      fontsize=16)\n\nfig\n\n(Image: Separable GP Predictions)\n\nThe left panel shows the predicted mean surface smoothly interpolating the training data (white points). The right panel shows prediction uncertainty - low near training points and higher in regions with sparse data.","category":"section"},{"location":"examples/multivariate/#Key-Concepts","page":"Multivariate Inputs (ARD)","title":"Key Concepts","text":"","category":"section"},{"location":"examples/multivariate/#Isotropic-vs-Separable-(ARD)-Kernels","page":"Multivariate Inputs (ARD)","title":"Isotropic vs Separable (ARD) Kernels","text":"Property Isotropic (GP) Separable (GPsep)\nLengthscale Single d for all dimensions Vector d[1:m] per dimension\nAssumption Function varies equally in all directions Function can vary differently per dimension\nParameters 2 (d, g) m+1 (d[1], ..., d[m], g)\nUse when Dimensions are interchangeable Dimensions have different relevance","category":"section"},{"location":"examples/multivariate/#When-to-Use-ARD","page":"Multivariate Inputs (ARD)","title":"When to Use ARD","text":"Use GPsep (separable/ARD) when:\n\nInput dimensions have different scales or units - e.g., mixing temperature (°C) and concentration (mol/L)\nSome inputs may be irrelevant - ARD can discover this via large lengthscales\nPhysical intuition suggests anisotropy - directional processes, hierarchical effects\nVariable importance is of interest - lengthscale ratios indicate relative sensitivity\n\nUse GP (isotropic) when:\n\nDimensions are exchangeable - e.g., spatial coordinates with no preferred direction\nLimited data - fewer parameters means less overfitting risk\nComputational efficiency - isotropic kernels can be slightly faster","category":"section"},{"location":"examples/multivariate/#Practical-Notes","page":"Multivariate Inputs (ARD)","title":"Practical Notes","text":"Initialization: Both dimensions start with the same lengthscale. The optimizer then differentiates them based on the data.\nIdentifiability: With very little data, ARD lengthscales may not be well-determined. Consider isotropic GP if you have fewer than ~10 points per dimension.\nScaling inputs: For best results, normalize inputs to similar ranges before fitting. This prevents numerical issues and makes lengthscale magnitudes comparable.\nNugget estimation: The nugget g captures observation noise plus any unmodeled small-scale variation. A larger nugget indicates noisier data or model misspecification.","category":"section"},{"location":"examples/demo/#Local-Approximate-GP-Demo","page":"Local Approximate GP Demo","title":"Local Approximate GP Demo","text":"This example demonstrates the core laGP.jl workflow, adapted from the R laGP package demo.","category":"section"},{"location":"examples/demo/#Overview","page":"Local Approximate GP Demo","title":"Overview","text":"We will:\n\nBuild a full GP model on a 2D test function\nMake predictions with local approximate GP\nCompare different acquisition methods","category":"section"},{"location":"examples/demo/#Setup","page":"Local Approximate GP Demo","title":"Setup","text":"using laGP\nusing Random\nusing LatinHypercubeSampling\nusing Statistics: mean\n\nRandom.seed!(42)","category":"section"},{"location":"examples/demo/#Test-Function","page":"Local Approximate GP Demo","title":"Test Function","text":"We use a modified Ackley-like function with interesting local structure:\n\nfunction f2d(X::Matrix)\n    x = X[:, 1]\n    y = X[:, 2]\n\n    g(z) = exp.(-(z .- 1).^2) .+ exp.(-0.8 .* (z .+ 1).^2) .- 0.05 .* sin.(8 .* (z .+ 0.1))\n\n    return -g(x) .* g(y)\nend\n\nThe function has two peaks and interesting local structure:\n\n(Image: True Function)","category":"section"},{"location":"examples/demo/#Generate-Training-Data","page":"Local Approximate GP Demo","title":"Generate Training Data","text":"Create a Latin Hypercube design:\n\nn_train = 100\nplan, _ = LHCoptim(n_train, 2, 10)\nX_train = Matrix{Float64}(plan ./ n_train)\n\n# Scale to [-2, 2] for f2d\nX_scaled = 4.0 .* (X_train .- 0.5)\nZ_train = f2d(X_scaled)\n\nprintln(\"Training data: $(n_train) points\")\nprintln(\"Response range: [$(minimum(Z_train)), $(maximum(Z_train))]\")","category":"section"},{"location":"examples/demo/#Create-Test-Grid","page":"Local Approximate GP Demo","title":"Create Test Grid","text":"n_test_side = 100\nx_test = range(0.0, 1.0, length=n_test_side)\nX_test = Matrix{Float64}(undef, n_test_side^2, 2)\n\nidx = 1\nfor j in 1:n_test_side\n    for i in 1:n_test_side\n        X_test[idx, 1] = x_test[i]\n        X_test[idx, 2] = x_test[j]\n        idx += 1\n    end\nend\n\nprintln(\"Test grid: $(n_test_side^2) points\")","category":"section"},{"location":"examples/demo/#Full-GP-Model","page":"Local Approximate GP Demo","title":"Full GP Model","text":"First, build a full GP for comparison:\n\n# Estimate hyperparameters\nd_range = darg(X_train)\ng_range = garg(Z_train)\n\nprintln(\"Lengthscale range: $(d_range.min) to $(d_range.max)\")\nprintln(\"Nugget range: $(g_range.min) to $(g_range.max)\")\n\n# Create and fit GP\ngp = new_gp(X_train, Z_train, d_range.start, g_range.start)\njmle_gp(gp; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Optimized: d=$(gp.d), g=$(gp.g)\")\nprintln(\"Log-likelihood: $(llik_gp(gp))\")\n\n# Predict on test grid\npred_full = pred_gp(gp, X_test; lite=true)\n\nThe full GP provides smooth predictions over the entire domain:\n\n(Image: GP Mean Surface)\n\nThe variance surface shows uncertainty, which is lowest near training data:\n\n(Image: GP Variance Surface)","category":"section"},{"location":"examples/demo/#Local-Approximate-GP","page":"Local Approximate GP Demo","title":"Local Approximate GP","text":"Now use aGP with different acquisition methods:\n\n# ALC method (best accuracy)\nresult_alc = agp(X_train, Z_train, X_test;\n    start=6, endpt=50,\n    d=(start=gp.d, mle=false),\n    g=(start=gp.g, mle=false),\n    method=:alc\n)\n\n# NN method (fastest)\nresult_nn = agp(X_train, Z_train, X_test;\n    start=6, endpt=50,\n    d=(start=gp.d, mle=false),\n    g=(start=gp.g, mle=false),\n    method=:nn\n)\n\nThe aGP predictions are nearly indistinguishable from the full GP:\n\n(Image: aGP Predictions)","category":"section"},{"location":"examples/demo/#Compare-Results","page":"Local Approximate GP Demo","title":"Compare Results","text":"# True values for comparison\ntrue_vals = f2d(4.0 .* (X_test .- 0.5))\n\n# RMSE comparison\nrmse_full = sqrt(mean((pred_full.mean .- true_vals).^2))\nrmse_alc = sqrt(mean((result_alc.mean .- true_vals).^2))\nrmse_nn = sqrt(mean((result_nn.mean .- true_vals).^2))\n\nprintln(\"RMSE Comparison:\")\nprintln(\"  Full GP: $(round(rmse_full, digits=6))\")\nprintln(\"  aGP ALC: $(round(rmse_alc, digits=6))\")\nprintln(\"  aGP NN:  $(round(rmse_nn, digits=6))\")\n\nSide-by-side comparison of full GP vs local approximate GP predictions:\n\n(Image: Full GP vs aGP Comparison)","category":"section"},{"location":"examples/demo/#Local-Design-Visualization","page":"Local Approximate GP Demo","title":"Local Design Visualization","text":"Examine which points are selected for a specific prediction location:\n\n# Single prediction at center\nXref = [0.5, 0.5]\nlagp_result = lagp(Xref, 6, 30, X_train, Z_train;\n    d=gp.d, g=gp.g, method=:alc\n)\n\nprintln(\"Local design size: $(length(lagp_result.indices))\")\nprintln(\"Prediction: mean=$(lagp_result.mean), var=$(lagp_result.var)\")\n\nThe local design shows which training points were selected for predicting at the center:\n\n(Image: Local Design Selection)\n\nThe test function itself has interesting structure with constraints:\n\n(Image: Constrained Problem)","category":"section"},{"location":"examples/demo/#Key-Observations","page":"Local Approximate GP Demo","title":"Key Observations","text":"Full GP vs Local: For smooth functions, full GP and aGP produce similar predictions\nALC vs NN: ALC typically provides better accuracy by selecting informative points\nComputation: aGP scales better for large datasets (local designs are small)\nHyperparameters: Using MLE-optimized parameters from a full GP subset works well","category":"section"},{"location":"examples/demo/#Next-Steps","page":"Local Approximate GP Demo","title":"Next Steps","text":"Try different start and endpt values\nEnable local MLE with d=(start=..., mle=true)\nUse :mspe method for a balance of speed and accuracy","category":"section"},{"location":"#laGP.jl","page":"Home","title":"laGP.jl","text":"Local Approximate Gaussian Process Regression for Julia","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"laGP.jl is a Julia implementation of Local Approximate Gaussian Process (laGP) regression, ported from the R laGP package by Robert Gramacy. It enables scalable GP predictions for large datasets by building local GP models at each prediction point.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Scalable Predictions: Handle datasets with thousands of observations by building local GP models\nAbstractGPs.jl Backend: Built on the JuliaGaussianProcesses ecosystem for robust GP computations\nIsotropic & Separable Kernels: Single lengthscale or per-dimension ARD lengthscales\nAcquisition Functions: ALC (Active Learning Cohn) and MSPE for intelligent point selection\nMLE with Priors: Maximum likelihood estimation with Inverse-Gamma priors (MAP)\nMulti-threaded: Parallel predictions across test points","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/joshualeond/laGP.jl\")","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using laGP\nusing Random\n\nRandom.seed!(42)\n\n# Generate training data\nn = 100\nX = rand(n, 2)\nZ = sin.(2π * X[:, 1]) .* cos.(2π * X[:, 2]) + 0.1 * randn(n)\n\n# Estimate hyperparameters from data\nd_range = darg(X)\ng_range = garg(Z)\n\n# Create GP with initial parameters\ngp = new_gp(X, Z, d_range.start, g_range.start)\n\n# Optimize hyperparameters via joint MLE\njmle_gp(gp; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Optimized lengthscale: \", gp.d)\nprintln(\"Optimized nugget: \", gp.g)\n\n# Make predictions\nX_test = rand(10, 2)\npred = pred_gp(gp, X_test)\n\nprintln(\"Predictions: \", pred.mean)\nprintln(\"Variances: \", pred.s2)","category":"section"},{"location":"#Design-Matrix-Convention","page":"Home","title":"Design Matrix Convention","text":"All design matrices use rows as observations:\n\nX is n × m where n is number of points and m is dimensionality\nThis matches the R laGP convention","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\n    \"getting_started.md\",\n    \"theory.md\",\n    \"examples/demo.md\",\n    \"examples/sinusoidal.md\",\n    \"examples/surrogates.md\",\n    \"examples/satellite.md\",\n    \"api.md\",\n]\nDepth = 2","category":"section"},{"location":"#References","page":"Home","title":"References","text":"Gramacy, R. B. (2016). laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R. Journal of Statistical Software, 72(1), 1-46.\nGramacy, R. B. (2020). Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.","category":"section"},{"location":"examples/satellite/#Satellite-Drag-Modeling-Example","page":"Satellite Drag Modeling","title":"Satellite Drag Modeling Example","text":"This example demonstrates GP surrogate modeling for GRACE satellite drag coefficients, based on Chapter 2 of \"Surrogates\" by Robert Gramacy.","category":"section"},{"location":"examples/satellite/#Overview","page":"Satellite Drag Modeling","title":"Overview","text":"We will:\n\nLoad GRACE satellite drag coefficient data\nFit separable GPs for multiple atmospheric species\nMake predictions and compute RMSPE\nCombine species predictions for atmospheric mixture","category":"section"},{"location":"examples/satellite/#Background","page":"Satellite Drag Modeling","title":"Background","text":"The GRACE satellite mission requires accurate drag coefficient (Cd) predictions for orbit determination. The drag coefficient depends on:\n\nVelocity magnitude (Umag)\nSurface temperature (Ts)\nAtmospheric temperature (Ta)\nYaw and pitch angles (theta, phi)\nAccommodation coefficients (alphan, sigmat)\n\nData is available for 6 atmospheric species: He, O, O2, N, N2, H.","category":"section"},{"location":"examples/satellite/#Setup","page":"Satellite Drag Modeling","title":"Setup","text":"using laGP\nusing Downloads\nusing DelimitedFiles\nusing Random\nusing Statistics: mean\n\nRandom.seed!(42)\n\n# Variable names\nvar_names = [\"Umag\", \"Ts\", \"Ta\", \"theta\", \"phi\", \"alphan\", \"sigmat\"]\n\n# Atmospheric species\nspecies_list = [:He, :O, :O2, :N, :N2, :H]\n\n# Molecular masses (g/mol)\nmolecular_mass = Dict(\n    :He => 4.003, :O => 15.999, :O2 => 31.998,\n    :N => 14.007, :N2 => 28.014, :H => 1.008\n)","category":"section"},{"location":"examples/satellite/#Load-Data","page":"Satellite Drag Modeling","title":"Load Data","text":"Download data from the TPM repository:\n\nfunction load_grace_data(species::Symbol, n::Int)\n    url = \"https://bitbucket.org/gramacylab/tpm/raw/master/data/GRACE/CD_GRACE_$(n)_$(species).csv\"\n    io = IOBuffer()\n    Downloads.download(url, io)\n    seekstart(io)\n    content = String(take!(io))\n\n    lines = split(content, '\\n')\n    data_rows = Float64[]\n    for line in lines[2:end]\n        if !isempty(strip(line))\n            values = parse.(Float64, split(line, ','))\n            append!(data_rows, values)\n        end\n    end\n\n    n_cols = length(split(lines[1], ','))\n    return reshape(data_rows, n_cols, :)'\nend\n\n# Load training (n=1000) and test (n=100) data\ntrain_data = Dict(s => load_grace_data(s, 1000) for s in species_list)\ntest_data = Dict(s => load_grace_data(s, 100) for s in species_list)\n\nprintln(\"Loaded data for $(length(species_list)) species\")\nprintln(\"Training: 1000 points, Test: 100 points\")","category":"section"},{"location":"examples/satellite/#Normalize-Data","page":"Satellite Drag Modeling","title":"Normalize Data","text":"function normalize_data(train, test)\n    X_train_raw = train[:, 1:7]\n    X_test_raw = test[:, 1:7]\n    Y_train = train[:, 8]\n    Y_test = test[:, 8]\n\n    # Global normalization\n    ranges = [(minimum(vcat(X_train_raw[:,j], X_test_raw[:,j])),\n               maximum(vcat(X_train_raw[:,j], X_test_raw[:,j]))) for j in 1:7]\n\n    X_train = similar(X_train_raw)\n    X_test = similar(X_test_raw)\n    for j in 1:7\n        X_train[:, j] = (X_train_raw[:, j] .- ranges[j][1]) ./ (ranges[j][2] - ranges[j][1])\n        X_test[:, j] = (X_test_raw[:, j] .- ranges[j][1]) ./ (ranges[j][2] - ranges[j][1])\n    end\n\n    return X_train, X_test, Y_train, Y_test\nend\n\nnormalized = Dict(s => normalize_data(train_data[s], test_data[s]) for s in species_list)","category":"section"},{"location":"examples/satellite/#Fit-Separable-GPs","page":"Satellite Drag Modeling","title":"Fit Separable GPs","text":"gp_models = Dict{Symbol, GPsep{Float64}}()\npredictions = Dict{Symbol, NamedTuple}()\nrmspe_values = Dict{Symbol, Float64}()\n\nfor species in species_list\n    X_train, X_test, Y_train, Y_test = normalized[species]\n\n    # Get hyperparameter ranges\n    d_range = darg_sep(X_train)\n    g_range = garg(Y_train)\n\n    d_start = [r.start for r in d_range.ranges]\n    d_ranges = [(r.min, r.max) for r in d_range.ranges]\n\n    # Fit GP\n    gp = new_gp_sep(X_train, Y_train, d_start, g_range.start)\n    jmle_gp_sep(gp; drange=d_ranges, grange=(g_range.min, g_range.max))\n\n    # Predict\n    pred = pred_gp_sep(gp, X_test; lite=true)\n\n    # RMSPE\n    pct_errors = ((pred.mean .- Y_test) ./ Y_test) .* 100\n    rmspe = sqrt(mean(pct_errors.^2))\n\n    gp_models[species] = gp\n    predictions[species] = (mean=pred.mean, Y_test=Y_test)\n    rmspe_values[species] = rmspe\n\n    println(\"$species RMSPE: $(round(rmspe, digits=3))%\")\nend\n\nRMSPE comparison across species:\n\n(Image: RMSPE by Species)\n\nParity plot for Helium predictions:\n\n(Image: Helium Parity Plot)","category":"section"},{"location":"examples/satellite/#Atmospheric-Mixture","page":"Satellite Drag Modeling","title":"Atmospheric Mixture","text":"Combine species predictions using mole fraction weighting:\n\n# Example mole fractions at ~400 km altitude\nmole_fractions = Dict(\n    :O => 0.70, :N2 => 0.15, :He => 0.08,\n    :O2 => 0.04, :N => 0.02, :H => 0.01\n)\n\n# Mass-weighted mixture\nnumerator = zeros(100)\ndenominator = zeros(100)\ntrue_numerator = zeros(100)\n\nfor species in species_list\n    weight = mole_fractions[species] * molecular_mass[species]\n    numerator .+= predictions[species].mean .* weight\n    true_numerator .+= predictions[species].Y_test .* weight\n    denominator .+= weight\nend\n\nCd_mix_pred = numerator ./ denominator\nCd_mix_true = true_numerator ./ denominator\n\n# Mixture RMSPE\npct_errors_mix = ((Cd_mix_pred .- Cd_mix_true) ./ Cd_mix_true) .* 100\nrmspe_mixture = sqrt(mean(pct_errors_mix.^2))\n\nprintln(\"Mixture RMSPE: $(round(rmspe_mixture, digits=3))%\")\n\nParity plot for the atmospheric mixture predictions:\n\n(Image: Mixture Parity Plot)","category":"section"},{"location":"examples/satellite/#Lengthscale-Analysis","page":"Satellite Drag Modeling","title":"Lengthscale Analysis","text":"println(\"\\nLengthscales by variable (smaller = more important):\")\nprintln(rpad(\"Species\", 8), join([rpad(v, 10) for v in var_names]))\n\nfor species in species_list\n    d = gp_models[species].d\n    vals = join([rpad(round(d[j], sigdigits=3), 10) for j in 1:7])\n    println(rpad(string(species), 8), vals)\nend\n\n# Average importance\navg_d = zeros(7)\nfor species in species_list\n    avg_d .+= gp_models[species].d\nend\navg_d ./= length(species_list)\n\nsorted_idx = sortperm(avg_d)\nprintln(\"\\nMost influential inputs:\")\nfor i in 1:3\n    j = sorted_idx[i]\n    println(\"  $(var_names[j]): avg lengthscale = $(round(avg_d[j], sigdigits=3))\")\nend\n\nLengthscales by species reveal which inputs matter most:\n\n(Image: Lengthscales by Species)","category":"section"},{"location":"examples/satellite/#Main-Effects-(for-Helium)","page":"Satellite Drag Modeling","title":"Main Effects (for Helium)","text":"gp_he = gp_models[:He]\nbaseline = fill(0.5, 7)\n\nn_me = 100\nx_me = range(0.0, 1.0, length=n_me)\n\nmain_effects_he = Matrix{Float64}(undef, n_me, 7)\n\nfor j in 1:7\n    XX = repeat(baseline', n_me, 1)\n    XX[:, j] = collect(x_me)\n    pred = pred_gp_sep(gp_he, XX; lite=true)\n    main_effects_he[:, j] = pred.mean\nend\n\nprintln(\"\\nHe sensitivity (effect range):\")\nfor j in 1:7\n    range_j = maximum(main_effects_he[:, j]) - minimum(main_effects_he[:, j])\n    println(\"  $(var_names[j]): $(round(range_j, digits=4))\")\nend\n\nMain effects for Helium show variable sensitivities:\n\n(Image: Main Effects for He)","category":"section"},{"location":"examples/satellite/#Visualization-(with-CairoMakie)","page":"Satellite Drag Modeling","title":"Visualization (with CairoMakie)","text":"using CairoMakie\n\n# Parity plot for Helium\nfig = Figure(size=(500, 450))\nax = Axis(fig[1, 1],\n    xlabel=\"True Cd\",\n    ylabel=\"Predicted Cd\",\n    title=\"He: Predicted vs True (RMSPE=$(round(rmspe_values[:He], digits=2))%)\",\n    aspect=DataAspect()\n)\n\nY_test = predictions[:He].Y_test\npred_he = predictions[:He].mean\n\nlims = (minimum(vcat(Y_test, pred_he)) - 0.1, maximum(vcat(Y_test, pred_he)) + 0.1)\nlines!(ax, [lims[1], lims[2]], [lims[1], lims[2]], color=:red, linewidth=2, linestyle=:dash)\nscatter!(ax, Y_test, pred_he, color=:blue, markersize=8, alpha=0.7)\n\nxlims!(ax, lims...)\nylims!(ax, lims...)\n\nfig","category":"section"},{"location":"examples/satellite/#Key-Findings","page":"Satellite Drag Modeling","title":"Key Findings","text":"Species variation: Different species have different drag characteristics\nInput importance: Velocity magnitude and accommodation coefficients are most influential\nMixture benefit: Mixture RMSPE is often lower due to averaging effects\nSeparable advantage: Per-dimension lengthscales reveal input sensitivities\nScalability: GPs trained on 1000 points predict accurately on held-out data","category":"section"},{"location":"examples/sinusoidal/#Posterior-Sampling-Example","page":"Posterior Sampling","title":"Posterior Sampling Example","text":"This example demonstrates GP posterior sampling using the full covariance matrix, fitting a sparse sinusoidal dataset.","category":"section"},{"location":"examples/sinusoidal/#Overview","page":"Posterior Sampling","title":"Overview","text":"We will:\n\nFit an isotropic GP to sparse sinusoidal data\nUse pred_gp(lite=false) to get the full posterior covariance\nDraw posterior samples from the GP using the Student-t distribution\nVisualize the posterior uncertainty","category":"section"},{"location":"examples/sinusoidal/#Setup","page":"Posterior Sampling","title":"Setup","text":"using laGP\nusing Distributions\nusing PDMats\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(42)","category":"section"},{"location":"examples/sinusoidal/#Generate-Training-Data","page":"Posterior Sampling","title":"Generate Training Data","text":"Create sparse observations from sin(x):\n\n# Sparse training points over [0, 2π]\nX_train = reshape(collect(range(0, 2π, length=6)), :, 1)\nY_train = sin.(X_train[:, 1])\n\nprintln(\"Training data:\")\nprintln(\"  X: \", vec(X_train))\nprintln(\"  Y: \", Y_train)","category":"section"},{"location":"examples/sinusoidal/#Fit-Isotropic-GP","page":"Posterior Sampling","title":"Fit Isotropic GP","text":"# Initial hyperparameters\nd_init = 2.0   # lengthscale\ng_init = 1e-6  # small nugget for near-interpolation\n\n# Create GP\ngp = new_gp(X_train, Y_train, d_init, g_init)\n\n# MLE for lengthscale only (keep small nugget fixed)\nmle_gp(gp, :d; tmax=20.0)\n\nprintln(\"Optimized hyperparameters:\")\nprintln(\"  d = \", gp.d)\nprintln(\"  g = \", gp.g)\nprintln(\"  log-likelihood = \", llik_gp(gp))","category":"section"},{"location":"examples/sinusoidal/#Full-Posterior-Prediction","page":"Posterior Sampling","title":"Full Posterior Prediction","text":"Get the full covariance matrix for sampling:\n\n# Dense test grid\nxx = collect(range(-1, 2π + 1, length=499))\nXX = reshape(xx, :, 1)\n\n# Get full posterior (lite=false returns GPPredictionFull)\npred_full = pred_gp(gp, XX; lite=false)\n\nprintln(\"Prediction:\")\nprintln(\"  Test points: \", length(xx))\nprintln(\"  Sigma size: \", size(pred_full.Sigma))","category":"section"},{"location":"examples/sinusoidal/#Draw-Posterior-Samples","page":"Posterior Sampling","title":"Draw Posterior Samples","text":"The laGP package uses a concentrated (profile) likelihood to estimate the variance parameter τ². This introduces additional uncertainty that should be captured using a Student-t distribution rather than a Normal distribution:\n\n# Draw posterior samples using MvTDist (Student-t)\n# This mirrors R's rmvt and correctly accounts for uncertainty in τ² estimation\nmvt = MvTDist(pred_full.df, pred_full.mean, PDMat(Symmetric(pred_full.Sigma)))\n\nn_samples = 100\nsamples = rand(mvt, n_samples)\n\nprintln(\"Posterior samples: \", size(samples))","category":"section"},{"location":"examples/sinusoidal/#Why-Student-t?","page":"Posterior Sampling","title":"Why Student-t?","text":"The concentrated likelihood marginalizes out τ² analytically, which means the posterior predictive distribution is Student-t with df = n (number of training observations). Using MvNormal would underestimate uncertainty, especially with small training sets.","category":"section"},{"location":"examples/sinusoidal/#Visualization-(with-CairoMakie)","page":"Posterior Sampling","title":"Visualization (with CairoMakie)","text":"using CairoMakie\n\nfig = Figure(size=(700, 500))\nax = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"Y(x) | θ̂\",\n    title=\"Simple Sinusoidal Example: GP Posterior Samples\"\n)\n\n# Draw posterior samples (gray, semi-transparent)\nfor i in 1:n_samples\n    lines!(ax, xx, samples[:, i], color=(:gray, 0.3), linewidth=0.5)\nend\n\n# Draw posterior mean (blue)\nlines!(ax, xx, pred_full.mean, color=:blue, linewidth=2, label=\"Posterior mean\")\n\n# Draw true function (dashed green)\nlines!(ax, xx, sin.(xx), color=:green, linewidth=1.5, linestyle=:dash, label=\"sin(x)\")\n\n# Draw training points (black circles)\nscatter!(ax, vec(X_train), Y_train, color=:black, markersize=12, label=\"Training data\")\n\naxislegend(ax, position=:lb)\nxlims!(ax, -1, 2π + 1)\nylims!(ax, -2.0, 2.0)\n\nfig\n\nThe resulting visualization shows the posterior samples with the GP mean and true function:\n\n(Image: GP Posterior Samples)","category":"section"},{"location":"examples/sinusoidal/#Key-Concepts","page":"Posterior Sampling","title":"Key Concepts","text":"","category":"section"},{"location":"examples/sinusoidal/#Posterior-Mean-and-Variance","page":"Posterior Sampling","title":"Posterior Mean and Variance","text":"The posterior mean interpolates the training data (due to small nugget), while the variance:\n\nIs near zero at training points\nIncreases between and beyond training points\nReflects uncertainty about the true function","category":"section"},{"location":"examples/sinusoidal/#Posterior-Samples","page":"Posterior Sampling","title":"Posterior Samples","text":"Each sample represents a plausible function consistent with:\n\nThe training data\nThe GP prior (smoothness encoded by kernel)\nThe estimated hyperparameters","category":"section"},{"location":"examples/sinusoidal/#Practical-Notes","page":"Posterior Sampling","title":"Practical Notes","text":"Nugget size: A very small nugget (1e-6) gives near-interpolation. Increase for noisy data.\nMemory: Full covariance requires O(n²) storage. For large test sets, use lite=true.\nStudent-t vs Normal: Always use MvTDist for posterior samples when using concentrated likelihood. The degrees of freedom df = n (training set size).","category":"section"},{"location":"examples/sinusoidal/#Credible-Intervals","page":"Posterior Sampling","title":"Credible Intervals","text":"Instead of sampling, you can compute pointwise intervals:\n\nusing Distributions: TDist, quantile\n\n# 95% credible intervals using Student-t distribution\nt_dist = TDist(pred_full.df)\nt_crit = quantile(t_dist, 0.975)\n\n# Standard deviation at each point\nstd_pred = sqrt.(diag(pred_full.Sigma))\n\n# Intervals\nlower = pred_full.mean .- t_crit .* std_pred\nupper = pred_full.mean .+ t_crit .* std_pred\n\nThis is more efficient than sampling when you only need intervals.","category":"section"}]
}
