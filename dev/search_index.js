var documenterSearchIndex = {"docs":
[{"location":"examples/surrogates/#Wing-Weight-Surrogate-Example","page":"Wing Weight Surrogate","title":"Wing Weight Surrogate Example","text":"This example demonstrates GP surrogate modeling using the aircraft wing weight simulator from Chapter 1 of \"Surrogates\" by Robert Gramacy.","category":"section"},{"location":"examples/surrogates/#Overview","page":"Wing Weight Surrogate","title":"Overview","text":"We will:\n\nBuild a GP surrogate for a 9-dimensional wing weight function\nCompare isotropic vs separable (ARD) GP models\nPerform main effects/sensitivity analysis\nVisualize predictions on 2D slices","category":"section"},{"location":"examples/surrogates/#Setup","page":"Wing Weight Surrogate","title":"Setup","text":"using laGP\nusing Random\nusing LatinHypercubeSampling\nusing Statistics: mean\n\nRandom.seed!(42)","category":"section"},{"location":"examples/surrogates/#Wing-Weight-Simulator","page":"Wing Weight Surrogate","title":"Wing Weight Simulator","text":"The simulator computes aircraft wing structural weight based on 9 design parameters:\n\nfunction wingwt(; Sw=0.48, Wfw=0.4, A=0.38, L=0.5, q=0.62,\n                  l=0.344, Rtc=0.4, Nz=0.37, Wdg=0.38)\n    # Transform coded [0,1] inputs to natural units\n    Sw_nat = Sw * (200 - 150) + 150       # Wing area (ft²)\n    Wfw_nat = Wfw * (300 - 220) + 220     # Fuel weight (lb)\n    A_nat = A * (10 - 6) + 6              # Aspect ratio\n    L_nat = (L * 20 - 10) * π / 180       # Sweep angle (rad)\n    q_nat = q * (45 - 16) + 16            # Dynamic pressure (lb/ft²)\n    l_nat = l * 0.5 + 0.5                 # Taper ratio\n    Rtc_nat = Rtc * 0.1 + 0.08            # Thickness ratio\n    Nz_nat = Nz * 3.5 + 2.5               # Load factor\n    Wdg_nat = Wdg * 800 + 1700            # Gross weight (lb)\n\n    # Wing weight formula\n    W = 0.036 * Sw_nat^0.758 * Wfw_nat^0.0035\n    W *= (A_nat / cos(L_nat)^2)^0.6\n    W *= q_nat^0.006 * l_nat^0.04\n    W *= (100 * Rtc_nat / cos(L_nat))^(-0.3)\n    W *= (Nz_nat * Wdg_nat)^0.49\n\n    return W\nend\n\n# Variable names\nvar_names = [\"Sw\", \"Wfw\", \"A\", \"L\", \"q\", \"l\", \"Rtc\", \"Nz\", \"Wdg\"]\n\n# Cessna C172 baseline\nbaseline = [0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38]","category":"section"},{"location":"examples/surrogates/#Generate-Training-Data","page":"Wing Weight Surrogate","title":"Generate Training Data","text":"Use Latin Hypercube Sampling for space-filling design:\n\nn = 1000\nplan, _ = LHCoptim(n, 9, 10)\nX = Matrix{Float64}(plan ./ n)\n\n# Evaluate simulator\nY = [wingwt(Sw=X[i,1], Wfw=X[i,2], A=X[i,3], L=X[i,4],\n            q=X[i,5], l=X[i,6], Rtc=X[i,7], Nz=X[i,8], Wdg=X[i,9])\n     for i in 1:n]\n\nprintln(\"Training data: $n points in 9 dimensions\")\nprintln(\"Response range: [$(round(minimum(Y), digits=2)), $(round(maximum(Y), digits=2))] lb\")\n\nExample response surfaces from Chapter 1:\n\n(Image: Banana Yield Function)\n\n(Image: RSM Function Gallery)\n\nThe Latin Hypercube design provides space-filling coverage:\n\n(Image: LHS Design Projection)","category":"section"},{"location":"examples/surrogates/#Fit-Isotropic-GP","page":"Wing Weight Surrogate","title":"Fit Isotropic GP","text":"# Get hyperparameter ranges\nd_range = darg(X)\ng_range = garg(Y)\n\n# Fit isotropic GP\ngp_iso = new_gp(X, Y, d_range.start, g_range.start)\njmle_gp(gp_iso; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Isotropic GP:\")\nprintln(\"  d = $(round(gp_iso.d, sigdigits=4))\")\nprintln(\"  g = $(round(gp_iso.g, sigdigits=4))\")\nprintln(\"  log-likelihood = $(round(llik_gp(gp_iso), digits=2))\")","category":"section"},{"location":"examples/surrogates/#Fit-Separable-GP","page":"Wing Weight Surrogate","title":"Fit Separable GP","text":"# Per-dimension lengthscale ranges\nd_range_sep = darg_sep(X)\n\nd_start_sep = [r.start for r in d_range_sep.ranges]\nd_ranges_sep = [(r.min, r.max) for r in d_range_sep.ranges]\n\n# Fit separable GP\ngp_sep = new_gp_sep(X, Y, d_start_sep, g_range.start)\njmle_gp_sep(gp_sep; drange=d_ranges_sep, grange=(g_range.min, g_range.max))\n\nprintln(\"Separable GP:\")\nprintln(\"  Lengthscales:\")\nfor (j, name) in enumerate(var_names)\n    println(\"    $name: $(round(gp_sep.d[j], sigdigits=4))\")\nend\nprintln(\"  g = $(round(gp_sep.g, sigdigits=4))\")\nprintln(\"  log-likelihood = $(round(llik_gp_sep(gp_sep), digits=2))\")","category":"section"},{"location":"examples/surrogates/#Compare-on-2D-Slice","page":"Wing Weight Surrogate","title":"Compare on 2D Slice","text":"Predict on A (aspect ratio) × Nz (load factor) slice:\n\nn_pred = 100\nx_pred = range(0.0, 1.0, length=n_pred)\n\n# Create prediction grid (vary A and Nz, fix others at baseline)\nXX = Matrix{Float64}(undef, n_pred^2, 9)\nfor i in 1:n_pred^2\n    XX[i, :] .= baseline\nend\n\nidx = 1\nfor nz in x_pred\n    for a in x_pred\n        XX[idx, 3] = a   # A\n        XX[idx, 8] = nz  # Nz\n        idx += 1\n    end\nend\n\n# Predictions\npred_iso = pred_gp(gp_iso, XX; lite=true)\npred_sep = pred_gp_sep(gp_sep, XX; lite=true)\n\n# True values for comparison\ntrue_vals = [wingwt(Sw=baseline[1], Wfw=baseline[2], A=a, L=baseline[4],\n                    q=baseline[5], l=baseline[6], Rtc=baseline[7], Nz=nz, Wdg=baseline[9])\n             for a in x_pred, nz in x_pred]\n\n# RMSE\nrmse_iso = sqrt(mean((vec(true_vals) .- pred_iso.mean).^2))\nrmse_sep = sqrt(mean((vec(true_vals) .- pred_sep.mean).^2))\n\nprintln(\"A × Nz slice RMSE:\")\nprintln(\"  Isotropic: $(round(rmse_iso, digits=4)) lb\")\nprintln(\"  Separable: $(round(rmse_sep, digits=4)) lb\")\n\n2D slice visualizations of the wing weight function:\n\n(Image: Wing Weight A×Nz Slice)\n\n(Image: Wing Weight λ×Wfw Slice)\n\nIsotropic vs Separable GP comparison on the A×Nz slice:\n\n(Image: Surrogate Comparison (A×Nz))\n\n(Image: Surrogate Comparison (λ×Wfw))","category":"section"},{"location":"examples/surrogates/#Main-Effects-Analysis","page":"Wing Weight Surrogate","title":"Main Effects Analysis","text":"Compute sensitivity of each input while holding others at baseline:\n\nn_me = 100\nx_me = range(0.0, 1.0, length=n_me)\n\nmain_effects = Matrix{Float64}(undef, n_me, 9)\n\nfor j in 1:9\n    # Create prediction matrix\n    XX_me = repeat(baseline', n_me, 1)\n    XX_me[:, j] = collect(x_me)\n\n    # Predict using separable GP\n    pred_me = pred_gp_sep(gp_sep, XX_me; lite=true)\n    main_effects[:, j] = pred_me.mean\nend\n\n# Report sensitivity (range of main effect)\nprintln(\"Variable sensitivity (range of main effect):\")\nfor j in 1:9\n    effect_range = maximum(main_effects[:, j]) - minimum(main_effects[:, j])\n    println(\"  $(var_names[j]): $(round(effect_range, digits=2)) lb\")\nend","category":"section"},{"location":"examples/surrogates/#Visualization-(with-CairoMakie)","page":"Wing Weight Surrogate","title":"Visualization (with CairoMakie)","text":"using CairoMakie\n\n# Main effects plot\nfig = Figure(size=(800, 500))\nax = Axis(fig[1, 1],\n    xlabel=\"Coded Input [0, 1]\",\n    ylabel=\"Wing Weight (lb)\",\n    title=\"Main Effects Analysis\"\n)\n\ncolors = [:blue, :red, :green, :orange, :purple, :brown, :pink, :gray, :cyan]\n\nfor j in 1:9\n    lines!(ax, collect(x_me), main_effects[:, j],\n           color=colors[j], linewidth=2, label=var_names[j])\nend\n\nLegend(fig[1, 2], ax, nbanks=1)\nfig\n\nThe main effects analysis reveals input sensitivities:\n\n(Image: Main Effects Analysis)","category":"section"},{"location":"examples/surrogates/#Key-Findings","page":"Wing Weight Surrogate","title":"Key Findings","text":"Most influential inputs: Sw (wing area), A (aspect ratio), Nz (load factor)\nLeast influential inputs: l (taper ratio), Wfw (fuel weight)\nIsotropic vs Separable: For smooth functions, both may converge to similar predictions\nLengthscale interpretation: Smaller lengthscale = more sensitive input","category":"section"},{"location":"examples/surrogates/#Surrogate-Benefits","page":"Wing Weight Surrogate","title":"Surrogate Benefits","text":"Speed: Surrogate predictions are instantaneous vs expensive simulations\nOptimization: Use GP for derivative-free optimization\nUncertainty: Predictive variance quantifies surrogate accuracy\nSensitivity: Lengthscales directly indicate input importance","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#Core-GP-Functions-(Isotropic)","page":"API Reference","title":"Core GP Functions (Isotropic)","text":"","category":"section"},{"location":"api/#Core-GP-Functions-(Separable)","page":"API Reference","title":"Core GP Functions (Separable)","text":"","category":"section"},{"location":"api/#MLE-Functions-(Isotropic)","page":"API Reference","title":"MLE Functions (Isotropic)","text":"","category":"section"},{"location":"api/#MLE-Functions-(Separable)","page":"API Reference","title":"MLE Functions (Separable)","text":"","category":"section"},{"location":"api/#AD-based-Gradient-Functions","page":"API Reference","title":"AD-based Gradient Functions","text":"","category":"section"},{"location":"api/#Acquisition-Functions","page":"API Reference","title":"Acquisition Functions","text":"","category":"section"},{"location":"api/#Local-GP-Functions","page":"API Reference","title":"Local GP Functions","text":"","category":"section"},{"location":"api/#Plotting-Functions","page":"API Reference","title":"Plotting Functions","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#laGP.GP","page":"API Reference","title":"laGP.GP","text":"GP{T<:Real, K}\n\nGaussian Process model backed by AbstractGPs.jl with isotropic squared-exponential kernel.\n\nThis type uses AbstractGPs for the posterior computation while preserving laGP-specific quantities needed for the concentrated likelihood formula.\n\nFields\n\nX::Matrix{T}: n x m design matrix (n observations, m dimensions)\nZ::Vector{T}: n response values\nkernel::K: Kernel from KernelFunctions.jl\nchol::Cholesky{T}: Cholesky factorization of K + g*I\nKiZ::Vector{T}: K \\ Z (precomputed for prediction)\nd::T: lengthscale parameter (laGP parameterization)\ng::T: nugget parameter\nphi::T: Z' * Ki * Z (used for variance scaling)\nldetK::T: log determinant of K (used for likelihood)\n\nNotes\n\nThe AbstractGPs posterior can be reconstructed from (X, Z, kernel, g) when needed. We cache the Cholesky and derived quantities for efficient repeated computations.\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.GPsep","page":"API Reference","title":"laGP.GPsep","text":"GPsep{T<:Real, K}\n\nSeparable Gaussian Process model backed by AbstractGPs.jl with anisotropic kernel.\n\nUses a vector of lengthscales (one per input dimension) to capture varying input sensitivities.\n\nFields\n\nX::Matrix{T}: n x m design matrix (n observations, m dimensions)\nZ::Vector{T}: n response values\nkernel::K: ARD kernel from KernelFunctions.jl\nchol::Cholesky{T}: Cholesky factorization of K + g*I\nKiZ::Vector{T}: K \\ Z (precomputed for prediction)\nd::Vector{T}: lengthscale parameters (m elements, one per dimension)\ng::T: nugget parameter\nphi::T: Z' * Ki * Z (used for variance scaling)\nldetK::T: log determinant of K (used for likelihood)\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.GPPrediction","page":"API Reference","title":"laGP.GPPrediction","text":"GPPrediction{T<:Real}\n\nResult of GP prediction.\n\nFields\n\nmean::Vector{T}: predicted mean values\ns2::Vector{T}: predicted variances (if lite=true) or full covariance\ndf::Int: degrees of freedom (n observations)\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.GPPredictionFull","page":"API Reference","title":"laGP.GPPredictionFull","text":"GPPredictionFull{T<:Real}\n\nResult of GP prediction with full covariance matrix.\n\nFields\n\nmean::Vector{T}: predicted mean values\nSigma::Matrix{T}: full posterior covariance matrix (ntest x ntest)\ndf::Int: degrees of freedom (n observations)\n\n\n\n\n\n","category":"type"},{"location":"api/#laGP.new_gp","page":"API Reference","title":"laGP.new_gp","text":"new_gp(X, Z, d, g)\n\nCreate a new Gaussian Process model using AbstractGPs.jl backend.\n\nArguments\n\nX::Matrix: n x m design matrix (n observations, m dimensions)\nZ::Vector: n response values\nd::Real: lengthscale parameter (laGP parameterization)\ng::Real: nugget parameter\n\nReturns\n\nGP: Gaussian Process model backed by AbstractGPs\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.pred_gp","page":"API Reference","title":"laGP.pred_gp","text":"pred_gp(gp, XX; lite=true)\n\nMake predictions at test locations XX using AbstractGPs-backed GP.\n\nArguments\n\ngp::GP: Gaussian Process model\nXX::Matrix: test locations (n_test x m)\nlite::Bool: if true, return only diagonal variances\n\nReturns\n\nGPPrediction: prediction results with mean, s2, and df\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.llik_gp","page":"API Reference","title":"laGP.llik_gp","text":"llik_gp(gp)\n\nCompute the log-likelihood of the GP.\n\nUses the concentrated likelihood formula from R laGP:     llik = -0.5 * (n * log(0.5 * phi) + ldetK)\n\nArguments\n\ngp::GP: Gaussian Process model\n\nReturns\n\nReal: log-likelihood value\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.dllik_gp","page":"API Reference","title":"laGP.dllik_gp","text":"dllik_gp(gp; dg=true, dd=true)\n\nCompute gradient of log-likelihood w.r.t. d (lengthscale) and g (nugget).\n\nArguments\n\ngp::GP: Gaussian Process model\ndg::Bool: compute gradient w.r.t. nugget g (default: true)\ndd::Bool: compute gradient w.r.t. lengthscale d (default: true)\n\nReturns\n\nNamedTuple: (dllg=..., dlld=...) gradients\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.update_gp!","page":"API Reference","title":"laGP.update_gp!","text":"update_gp!(gp; d=nothing, g=nothing)\n\nUpdate GP hyperparameters and recompute internal quantities.\n\nArguments\n\ngp::GP: Gaussian Process model\nd::Real: new lengthscale (optional)\ng::Real: new nugget (optional)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.new_gp_sep","page":"API Reference","title":"laGP.new_gp_sep","text":"new_gp_sep(X, Z, d, g)\n\nCreate a new separable Gaussian Process model using AbstractGPs.jl backend.\n\nArguments\n\nX::Matrix: n x m design matrix (n observations, m dimensions)\nZ::Vector: n response values\nd::Vector: lengthscale parameters (m elements, one per dimension)\ng::Real: nugget parameter\n\nReturns\n\nGPsep: Separable Gaussian Process model backed by AbstractGPs\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.pred_gp_sep","page":"API Reference","title":"laGP.pred_gp_sep","text":"pred_gp_sep(gp, XX; lite=true)\n\nMake predictions at test locations XX using AbstractGPs-backed separable GP.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\nXX::Matrix: test locations (n_test x m)\nlite::Bool: if true, return only diagonal variances\n\nReturns\n\nGPPrediction: prediction results with mean, s2, and df\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.llik_gp_sep","page":"API Reference","title":"laGP.llik_gp_sep","text":"llik_gp_sep(gp)\n\nCompute the log-likelihood of the GPsep.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\n\nReturns\n\nReal: log-likelihood value\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.dllik_gp_sep","page":"API Reference","title":"laGP.dllik_gp_sep","text":"dllik_gp_sep(gp; dg=true, dd=true)\n\nCompute gradient of log-likelihood w.r.t. d (lengthscales) and g (nugget).\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\ndg::Bool: compute gradient w.r.t. nugget g (default: true)\ndd::Bool: compute gradient w.r.t. lengthscales d (default: true)\n\nReturns\n\nNamedTuple: (dllg=..., dlld=...) gradients\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.update_gp_sep!","page":"API Reference","title":"laGP.update_gp_sep!","text":"update_gp_sep!(gp; d=nothing, g=nothing)\n\nUpdate GPsep hyperparameters and recompute internal quantities.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model\nd::Vector{Real}: new lengthscales (optional)\ng::Real: new nugget (optional)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.mle_gp","page":"API Reference","title":"laGP.mle_gp","text":"mle_gp(gp, param; tmax, tmin=sqrt(eps(T)))\n\nOptimize a single GP hyperparameter via maximum likelihood.\n\nArguments\n\ngp::GP: Gaussian Process model (modified in-place)\nparam::Symbol: parameter to optimize (:d or :g)\ntmax::Real: maximum value for parameter (required)\ntmin::Real: minimum value for parameter (default: sqrt(eps(T)), matching R's behavior)\n\nReturns\n\nNamedTuple: (d=..., g=..., its=..., msg=...) optimization result\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.jmle_gp","page":"API Reference","title":"laGP.jmle_gp","text":"jmle_gp(gp; drange, grange, maxit=100, verb=0, use_ad=false, dab=(3/2, nothing), gab=(3/2, nothing))\n\nJoint MLE optimization of d and g for GP.\n\nCan use either Zygote AD gradients (usead=true) or manual gradients (usead=false). Manual gradients are ~60x faster than Zygote AD and are the default.\n\nArguments\n\ngp::GP: Gaussian Process model (modified in-place)\ndrange::Tuple: (min, max) range for d\ngrange::Tuple: (min, max) range for g\nmaxit::Int: maximum iterations\nverb::Int: verbosity level\nuse_ad::Bool: use Zygote automatic differentiation for gradients (default: false for performance)\ndab::Tuple: (shape, scale) for d prior\ngab::Tuple: (shape, scale) for g prior\n\nReturns\n\nNamedTuple: (d=..., g=..., tot_its=..., msg=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.darg","page":"API Reference","title":"laGP.darg","text":"darg(X; d=nothing, ab=(3/2, nothing))\n\nCompute default arguments for lengthscale parameter.\n\nBased on pairwise distances in the design matrix X.\n\nArguments\n\nX::Matrix: design matrix\nd::Union{Nothing,Real}: user-specified d (optional)\nab::Tuple: (shape, scale) for Inverse-Gamma prior; if scale=nothing, computed from range\n\nReturns\n\nNamedTuple: (start=..., min=..., max=..., mle=..., ab=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.garg","page":"API Reference","title":"laGP.garg","text":"garg(Z; g=nothing, ab=(3/2, nothing))\n\nCompute default arguments for nugget parameter.\n\nBased on squared residuals from the mean.\n\nArguments\n\nZ::Vector: response values\ng::Union{Nothing,Real}: user-specified g (optional)\nab::Tuple: (shape, scale) for Inverse-Gamma prior; if scale=nothing, computed from range\n\nReturns\n\nNamedTuple: (start=..., min=..., max=..., mle=..., ab=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.mle_gp_sep","page":"API Reference","title":"laGP.mle_gp_sep","text":"mle_gp_sep(gp, param, dim; tmax, tmin=sqrt(eps(T)))\n\nOptimize a single hyperparameter of a separable GP via maximum likelihood.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model (modified in-place)\nparam::Symbol: parameter to optimize (:d or :g)\ndim::Int: dimension index for :d (ignored for :g)\ntmax::Real: maximum value for parameter (required)\ntmin::Real: minimum value for parameter (default: sqrt(eps(T)), matching R's behavior)\n\nReturns\n\nNamedTuple: (d=..., g=..., its=..., msg=...) optimization result\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.jmle_gp_sep","page":"API Reference","title":"laGP.jmle_gp_sep","text":"jmle_gp_sep(gp; drange, grange, maxit=100, verb=0, use_ad=false, dab=(3/2, nothing), gab=(3/2, nothing))\n\nJoint MLE optimization of lengthscales and nugget for GPsep.\n\nManual gradients are ~60x faster than Zygote AD and are the default.\n\nArguments\n\ngp::GPsep: Separable Gaussian Process model (modified in-place)\ndrange::Union{Tuple,Vector}: range for d parameters\ngrange::Tuple: (min, max) range for g\nmaxit::Int: maximum iterations\nverb::Int: verbosity level\nuse_ad::Bool: use Zygote automatic differentiation for gradients (default: false for performance)\ndab::Tuple: (shape, scale) for d prior\ngab::Tuple: (shape, scale) for g prior\n\nReturns\n\nNamedTuple: (d=..., g=..., tot_its=..., msg=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.darg_sep","page":"API Reference","title":"laGP.darg_sep","text":"darg_sep(X; d=nothing, ab=(3/2, nothing))\n\nCompute default arguments for lengthscale parameters (separable version).\n\nFollowing R's laGP convention, uses the TOTAL pairwise distance to compute initial ranges (same for all dimensions), then MLE finds per-dimension scaling.\n\nArguments\n\nX::Matrix: design matrix\nd::Union{Nothing,Vector}: user-specified d (optional)\nab::Tuple: (shape, scale) for Inverse-Gamma prior; if scale=nothing, computed from range\n\nReturns\n\nNamedTuple: (ranges=..., ab=...) where ranges is Vector of per-dimension NamedTuples\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.neg_llik_ad","page":"API Reference","title":"laGP.neg_llik_ad","text":"neg_llik_ad(params, X, Z; separable=false)\n\nCompute negative log-likelihood using a form suitable for Zygote AD.\n\nThis function recomputes the likelihood from scratch given parameters, making it compatible with automatic differentiation. It avoids in-place mutations for Zygote compatibility.\n\nArguments\n\nparams: For isotropic: [d, g]; for separable: [d..., g]\nX::Matrix: design matrix\nZ::Vector: response values\nseparable::Bool: if true, params contains per-dimension lengthscales\n\nReturns\n\nNegative log-likelihood value\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.dllik_ad","page":"API Reference","title":"laGP.dllik_ad","text":"dllik_ad(params, X, Z; separable=false)\n\nCompute gradient of log-likelihood using Zygote automatic differentiation.\n\nArguments\n\nparams: For isotropic: [d, g]; for separable: [d..., g]\nX::Matrix: design matrix\nZ::Vector: response values\nseparable::Bool: if true, params contains per-dimension lengthscales\n\nReturns\n\nGradient vector (same shape as params)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.alc_gp","page":"API Reference","title":"laGP.alc_gp","text":"alc_gp(gp, Xcand, Xref)\n\nCompute Active Learning Cohn (ALC) acquisition values.\n\nALC measures expected variance reduction at reference points Xref if we were to add each candidate point from Xcand to the design.\n\nArguments\n\ngp::GP: Gaussian Process model\nXcand::Matrix: candidate points (n_cand x m)\nXref::Matrix: reference points (n_ref x m)\n\nReturns\n\nVector: ALC values for each candidate point\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.mspe_gp","page":"API Reference","title":"laGP.mspe_gp","text":"mspe_gp(gp, Xcand, Xref)\n\nCompute Mean Squared Prediction Error (MSPE) acquisition values.\n\nMSPE is related to ALC and includes the current prediction variance.\n\nArguments\n\ngp::GP: Gaussian Process model\nXcand::Matrix: candidate points (n_cand x m)\nXref::Matrix: reference points (n_ref x m)\n\nReturns\n\nVector: MSPE values for each candidate point\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.lagp","page":"API Reference","title":"laGP.lagp","text":"lagp(Xref, start, endpt, X, Z; d, g, method=:alc, verb=0)\n\nLocal Approximate GP prediction at a single reference point.\n\nBuilds a local GP by starting with nearest neighbors and sequentially adding points that maximize the chosen acquisition function.\n\nArguments\n\nXref::Vector: single reference point (length m)\nstart::Int: initial number of nearest neighbors\nendpt::Int: final local design size\nX::Matrix: full training design (n x m)\nZ::Vector: full training responses\nd::Real: lengthscale parameter\ng::Real: nugget parameter\nmethod::Symbol: acquisition method (:alc, :mspe, or :nn)\nverb::Int: verbosity level\n\nReturns\n\nNamedTuple: (mean=..., var=..., df=..., indices=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.agp","page":"API Reference","title":"laGP.agp","text":"agp(X, Z, XX; start=6, endpt=50, d, g, method=:alc, verb=0, parallel=true)\n\nApproximate GP predictions at multiple reference points.\n\nCalls lagp for each row of XX, optionally in parallel using threads.\n\nArguments\n\nX::Matrix: training design (n x m)\nZ::Vector: training responses\nXX::Matrix: test/reference points (n_test x m)\nstart::Int: initial number of nearest neighbors\nendpt::Int: final local design size\nd::Union{Real,NamedTuple}: lengthscale parameter or (start, mle, min, max)\ng::Union{Real,NamedTuple}: nugget parameter or (start, mle, min, max)\nmethod::Symbol: acquisition method (:alc, :mspe, or :nn)\nverb::Int: verbosity level\nparallel::Bool: use multi-threading\n\nReturns\n\nNamedTuple: (mean=..., var=..., df=..., mle=...)\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.plot_gp_surface","page":"API Reference","title":"laGP.plot_gp_surface","text":"plot_gp_surface(gp, x_range, y_range; resolution=50, colormap=:viridis)\n\nCreate a heatmap of GP predictions over a 2D grid.\n\nArguments\n\ngp::GP: Gaussian Process model\nx_range::Tuple: (min, max) for x-axis\ny_range::Tuple: (min, max) for y-axis\nresolution::Int: number of points per dimension\ncolormap::Symbol: colormap for heatmap\n\nReturns\n\nFigure with heatmap and contour overlay\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.plot_gp_variance","page":"API Reference","title":"laGP.plot_gp_variance","text":"plot_gp_variance(gp, x_range, y_range; resolution=50, colormap=:plasma)\n\nCreate a heatmap of GP prediction variance over a 2D grid.\n\nArguments\n\ngp::GP: Gaussian Process model\nx_range::Tuple: (min, max) for x-axis\ny_range::Tuple: (min, max) for y-axis\nresolution::Int: number of points per dimension\ncolormap::Symbol: colormap for heatmap\n\nReturns\n\nFigure with variance heatmap\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.plot_local_design","page":"API Reference","title":"laGP.plot_local_design","text":"plot_local_design(X, Z, Xref, local_indices)\n\nVisualize the local design selection for a reference point.\n\nArguments\n\nX::Matrix: full training design (2D only)\nZ::Vector: training responses\nXref::Vector: reference point\nlocal_indices::Vector{Int}: indices of selected local design points\n\nReturns\n\nFigure showing training data, selected subset, and reference point\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.plot_agp_predictions","page":"API Reference","title":"laGP.plot_agp_predictions","text":"plot_agp_predictions(X, Z, XX, result; colormap=:viridis)\n\nVisualize aGP predictions at multiple reference points.\n\nArguments\n\nX::Matrix: training design (2D only)\nZ::Vector: training responses\nXX::Matrix: test/reference points\nresult::NamedTuple: output from agp function\n\nReturns\n\nFigure with predictions and uncertainty\n\n\n\n\n\n","category":"function"},{"location":"api/#laGP.contour_with_constraints","page":"API Reference","title":"laGP.contour_with_constraints","text":"contour_with_constraints(f, constraints, x_range, y_range;\n                         resolution=100, n_levels=10)\n\nCreate a contour plot of objective function with constraint boundaries.\n\nArguments\n\nf::Function: objective function (Matrix -> Vector of values)\nconstraints::Vector{Function}: constraint functions (each: Matrix -> Vector, feasible when ≤ 0)\nx_range::Tuple: (min, max) for x-axis\ny_range::Tuple: (min, max) for y-axis\nresolution::Int: number of points per dimension\nn_levels::Int: number of contour levels for objective\n\nReturns\n\nFigure with contours and constraint boundaries\n\n\n\n\n\n","category":"function"},{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"This page covers the mathematical background for laGP.jl.","category":"section"},{"location":"theory/#Gaussian-Process-Basics","page":"Theory","title":"Gaussian Process Basics","text":"A Gaussian Process (GP) is a collection of random variables, any finite subset of which have a joint Gaussian distribution. A GP is fully specified by its mean function m(x) and covariance function k(x x):\n\nf(x) sim mathcalGP(m(x) k(x x))\n\nGiven training data X Z where X is an n times m design matrix and Z is an n-vector of responses, the GP posterior at test points X_* is:\n\nbeginaligned\nmu_* = k(X_* X) K^-1 Z \nSigma_* = k(X_* X_*) - k(X_* X) K^-1 k(X X_*)\nendaligned\n\nwhere K = k(X X) + g I is the training covariance matrix with nugget g.","category":"section"},{"location":"theory/#Kernel-Parameterization","page":"Theory","title":"Kernel Parameterization","text":"","category":"section"},{"location":"theory/#Isotropic-Squared-Exponential","page":"Theory","title":"Isotropic Squared Exponential","text":"laGP uses the following parameterization:\n\nk(x y) = expleft(-fracx - y^2dright)\n\nwhere d is the lengthscale parameter.\n\nnote: KernelFunctions.jl Comparison\nKernelFunctions.jl uses: k(xy) = exp(-x-y^2(2ell^2))The mapping is: d = 2ell^2, so ell = sqrtd2","category":"section"},{"location":"theory/#Separable-(ARD)-Kernel","page":"Theory","title":"Separable (ARD) Kernel","text":"For separable/anisotropic GPs, each dimension has its own lengthscale:\n\nk(x y) = expleft(-sum_j=1^m frac(x_j - y_j)^2d_jright)\n\nThis allows the model to capture different smoothness in different input directions.","category":"section"},{"location":"theory/#Concentrated-Likelihood","page":"Theory","title":"Concentrated Likelihood","text":"laGP uses the concentrated (profile) log-likelihood:\n\nell = -frac12left(n logleft(fracphi2right) + logKright)\n\nwhere phi = Z^top K^-1 Z.\n\nThis formulation profiles out the variance parameter, leaving only d and g to optimize.","category":"section"},{"location":"theory/#Gradients","page":"Theory","title":"Gradients","text":"The gradient with respect to the nugget g:\n\nfracpartial ellpartial g = -frac12 texttr(K^-1) + fracn2phi (K^-1Z)^top (K^-1Z)\n\nThe gradient with respect to lengthscale d:\n\nfracpartial ellpartial d = -frac12 texttrleft(K^-1 fracpartial Kpartial dright) + fracn2phi Z^top K^-1 fracpartial Kpartial d K^-1 Z\n\nwhere for the isotropic kernel:\n\nfracpartial K_ijpartial d = K_ij fracx_i - x_j^2d^2","category":"section"},{"location":"theory/#Inverse-Gamma-Priors","page":"Theory","title":"Inverse-Gamma Priors","text":"MLE optimization uses Inverse-Gamma priors for regularization:\n\np(theta) propto theta^-a-1 expleft(-fracbthetaright)\n\nwhere a is the shape and b is the scale parameter.\n\nThe log-prior and its gradient:\n\nbeginaligned\nlog p(theta) = a log b - logGamma(a) - (a+1)logtheta - fracbtheta \nfracpartial log ppartial theta = -fraca+1theta + fracbtheta^2\nendaligned\n\nDefault values: a = 32, with b computed from the data-adaptive parameter ranges.","category":"section"},{"location":"theory/#Local-Approximate-GP","page":"Theory","title":"Local Approximate GP","text":"For large datasets, building a full GP is computationally prohibitive (O(n^3)). Local approximate GP builds a small local model for each prediction point.","category":"section"},{"location":"theory/#Algorithm","page":"Theory","title":"Algorithm","text":"For each prediction point x_*:\n\nInitialize: Select n_0 nearest neighbors to x_*\nIterate: Until local design has n_textend points:\nEvaluate acquisition function on all remaining candidates\nAdd the point that maximizes (ALC) or minimizes (MSPE) the criterion\nPredict: Make prediction using the local GP","category":"section"},{"location":"theory/#Active-Learning-Cohn-(ALC)","page":"Theory","title":"Active Learning Cohn (ALC)","text":"ALC measures the expected reduction in predictive variance at reference points if a candidate point were added:\n\ntextALC(x) = frac1n_textref sum_j=1^n_textref leftsigma^2(x_textrefj) - sigma^2_x(x_textrefj)right\n\nwhere sigma^2_x is the variance after adding point x to the design.\n\nHigher ALC values indicate points that would reduce prediction uncertainty more.","category":"section"},{"location":"theory/#Mean-Squared-Prediction-Error-(MSPE)","page":"Theory","title":"Mean Squared Prediction Error (MSPE)","text":"MSPE combines current variance with expected variance reduction:\n\ntextMSPE(x) = fracn+1n-1 barsigma^2 - fracn+1n(n-1) cdot fracn-2n cdot textALC(x)\n\nwhere barsigma^2 is the average predictive variance at reference points.\n\nLower MSPE values indicate better candidate points.","category":"section"},{"location":"theory/#Nearest-Neighbor-(NN)","page":"Theory","title":"Nearest Neighbor (NN)","text":"The simplest approach: just use the n_textend nearest neighbors. Fast but doesn't account for prediction location.","category":"section"},{"location":"theory/#Prediction-Variance-Scaling","page":"Theory","title":"Prediction Variance Scaling","text":"laGP scales the posterior variance using the Student-t distribution:\n\ns^2 = fracphin left(1 + g - k_*^top K^-1 k_*right)\n\nThis gives wider intervals than the standard GP formulation, accounting for hyperparameter uncertainty.\n\nThe degrees of freedom is n (number of training points).","category":"section"},{"location":"theory/#Data-Adaptive-Hyperparameter-Ranges","page":"Theory","title":"Data-Adaptive Hyperparameter Ranges","text":"","category":"section"},{"location":"theory/#Lengthscale-(darg)","page":"Theory","title":"Lengthscale (darg)","text":"The darg function computes ranges from pairwise distances:\n\nStart: 10th percentile of pairwise squared distances\nMin: Half the minimum non-zero distance (floored at sqrtepsilon)\nMax: Maximum pairwise distance","category":"section"},{"location":"theory/#Nugget-(garg)","page":"Theory","title":"Nugget (garg)","text":"The garg function computes ranges from response variability:\n\nStart: 2.5th percentile of squared residuals from mean\nMin: sqrtepsilon (machine epsilon)\nMax: Maximum squared residual","category":"section"},{"location":"theory/#References","page":"Theory","title":"References","text":"Gramacy, R. B. (2016). laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R. Journal of Statistical Software, 72(1), 1-46.\nGramacy, R. B. and Apley, D. W. (2015). Local Gaussian Process Approximation for Large Computer Experiments. Journal of Computational and Graphical Statistics, 24(2), 561-578.\nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This tutorial covers the basics of using laGP.jl for Gaussian Process regression.","category":"section"},{"location":"getting_started/#Creating-a-GP-Model","page":"Getting Started","title":"Creating a GP Model","text":"","category":"section"},{"location":"getting_started/#Isotropic-GP","page":"Getting Started","title":"Isotropic GP","text":"An isotropic GP uses a single lengthscale parameter for all dimensions:\n\nusing laGP\n\n# Training data: n observations, m dimensions\nX = rand(50, 2)  # 50 points in 2D\nZ = sin.(X[:, 1]) .+ cos.(X[:, 2]) + 0.1 * randn(50)\n\n# Initial hyperparameters\nd = 0.5  # lengthscale\ng = 0.01 # nugget (noise variance)\n\n# Create GP\ngp = new_gp(X, Z, d, g)","category":"section"},{"location":"getting_started/#Separable-GP-(ARD)","page":"Getting Started","title":"Separable GP (ARD)","text":"A separable GP uses per-dimension lengthscales:\n\n# Per-dimension lengthscales\nd = [0.5, 0.3]  # different lengthscale for each dimension\n\n# Create separable GP\ngp_sep = new_gp_sep(X, Z, d, g)","category":"section"},{"location":"getting_started/#Hyperparameter-Estimation","page":"Getting Started","title":"Hyperparameter Estimation","text":"","category":"section"},{"location":"getting_started/#Using-darg-and-garg","page":"Getting Started","title":"Using darg and garg","text":"The darg and garg functions compute sensible hyperparameter ranges from your data:\n\n# Get data-adaptive ranges\nd_range = darg(X)\ng_range = garg(Z)\n\nprintln(\"Lengthscale: start=$(d_range.start), range=[$(d_range.min), $(d_range.max)]\")\nprintln(\"Nugget: start=$(g_range.start), range=[$(g_range.min), $(g_range.max)]\")\n\n# Create GP with data-adaptive starting values\ngp = new_gp(X, Z, d_range.start, g_range.start)","category":"section"},{"location":"getting_started/#Maximum-Likelihood-Estimation","page":"Getting Started","title":"Maximum Likelihood Estimation","text":"Optimize hyperparameters using MLE:\n\n# Single parameter optimization\nmle_gp(gp, :d; tmax=d_range.max)  # optimize lengthscale only\n\n# Joint MLE of both parameters\njmle_gp(gp; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Optimized d: \", gp.d)\nprintln(\"Optimized g: \", gp.g)\n\nFor separable GPs:\n\nd_range_sep = darg_sep(X)\ngp_sep = new_gp_sep(X, Z, [r.start for r in d_range_sep.ranges], g_range.start)\n\njmle_gp_sep(gp_sep;\n    drange=[(r.min, r.max) for r in d_range_sep.ranges],\n    grange=(g_range.min, g_range.max)\n)\n\nprintln(\"Optimized lengthscales: \", gp_sep.d)","category":"section"},{"location":"getting_started/#Making-Predictions","page":"Getting Started","title":"Making Predictions","text":"","category":"section"},{"location":"getting_started/#Basic-Prediction","page":"Getting Started","title":"Basic Prediction","text":"# Test points\nX_test = rand(10, 2)\n\n# Predict (lite=true returns diagonal variances only)\npred = pred_gp(gp, X_test; lite=true)\n\nprintln(\"Mean: \", pred.mean)\nprintln(\"Variance: \", pred.s2)\nprintln(\"Degrees of freedom: \", pred.df)","category":"section"},{"location":"getting_started/#Full-Covariance-Matrix","page":"Getting Started","title":"Full Covariance Matrix","text":"For posterior sampling, get the full covariance:\n\n# Full prediction (lite=false returns full covariance matrix)\npred_full = pred_gp(gp, X_test; lite=false)\n\nprintln(\"Covariance matrix size: \", size(pred_full.Sigma))\n\n# Draw posterior samples\nusing Distributions\nmvn = MvNormal(pred_full.mean, Symmetric(pred_full.Sigma))\nsamples = rand(mvn, 100)  # 100 posterior samples","category":"section"},{"location":"getting_started/#Local-Approximate-GP","page":"Getting Started","title":"Local Approximate GP","text":"For large datasets, use local approximate GP:","category":"section"},{"location":"getting_started/#Single-Point-Prediction-with-lagp","page":"Getting Started","title":"Single Point Prediction with lagp","text":"# Large training set\nX_train = rand(5000, 2)\nZ_train = sin.(2π * X_train[:, 1]) .* cos.(2π * X_train[:, 2])\n\n# Reference point to predict\nXref = [0.5, 0.5]\n\n# Local GP prediction\nresult = lagp(Xref, 6, 50, X_train, Z_train;\n    d=0.1, g=1e-6,\n    method=:alc  # :alc, :mspe, or :nn\n)\n\nprintln(\"Prediction: \", result.mean)\nprintln(\"Variance: \", result.var)\nprintln(\"Local design indices: \", result.indices)","category":"section"},{"location":"getting_started/#Batch-Predictions-with-agp","page":"Getting Started","title":"Batch Predictions with agp","text":"# Multiple test points\nX_test = rand(100, 2)\n\n# Get hyperparameter ranges\nd_range = darg(X_train)\ng_range = garg(Z_train)\n\n# Approximate GP predictions\nresult = agp(X_train, Z_train, X_test;\n    start=6,   # initial nearest neighbors\n    endpt=50,  # final local design size\n    d=(start=d_range.start, mle=false),\n    g=(start=g_range.start, mle=false),\n    method=:alc,\n    parallel=true  # use multi-threading\n)\n\nprintln(\"Predictions shape: \", size(result.mean))","category":"section"},{"location":"getting_started/#Acquisition-Methods","page":"Getting Started","title":"Acquisition Methods","text":"laGP supports three acquisition methods for local design selection:\n\nMethod Description Use Case\n:alc Active Learning Cohn Best accuracy, slower\n:mspe Mean Squared Prediction Error Balance of speed/accuracy\n:nn Nearest Neighbors Fastest, less accurate\n\n# Compare methods\nresult_alc = agp(X_train, Z_train, X_test; method=:alc, ...)\nresult_mspe = agp(X_train, Z_train, X_test; method=:mspe, ...)\nresult_nn = agp(X_train, Z_train, X_test; method=:nn, ...)","category":"section"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"Theory: Mathematical background on GP kernels and acquisition functions\nExamples: Complete worked examples\nAPI Reference: Full function documentation","category":"section"},{"location":"examples/demo/#Local-Approximate-GP-Demo","page":"Local Approximate GP Demo","title":"Local Approximate GP Demo","text":"This example demonstrates the core laGP.jl workflow, adapted from the R laGP package demo.","category":"section"},{"location":"examples/demo/#Overview","page":"Local Approximate GP Demo","title":"Overview","text":"We will:\n\nBuild a full GP model on a 2D test function\nMake predictions with local approximate GP\nCompare different acquisition methods","category":"section"},{"location":"examples/demo/#Setup","page":"Local Approximate GP Demo","title":"Setup","text":"using laGP\nusing Random\nusing LatinHypercubeSampling\nusing Statistics: mean\n\nRandom.seed!(42)","category":"section"},{"location":"examples/demo/#Test-Function","page":"Local Approximate GP Demo","title":"Test Function","text":"We use a modified Ackley-like function with interesting local structure:\n\nfunction f2d(X::Matrix)\n    x = X[:, 1]\n    y = X[:, 2]\n\n    g(z) = exp.(-(z .- 1).^2) .+ exp.(-0.8 .* (z .+ 1).^2) .- 0.05 .* sin.(8 .* (z .+ 0.1))\n\n    return -g(x) .* g(y)\nend","category":"section"},{"location":"examples/demo/#Generate-Training-Data","page":"Local Approximate GP Demo","title":"Generate Training Data","text":"Create a Latin Hypercube design:\n\nn_train = 100\nplan, _ = LHCoptim(n_train, 2, 10)\nX_train = Matrix{Float64}(plan ./ n_train)\n\n# Scale to [-2, 2] for f2d\nX_scaled = 4.0 .* (X_train .- 0.5)\nZ_train = f2d(X_scaled)\n\nprintln(\"Training data: $(n_train) points\")\nprintln(\"Response range: [$(minimum(Z_train)), $(maximum(Z_train))]\")","category":"section"},{"location":"examples/demo/#Create-Test-Grid","page":"Local Approximate GP Demo","title":"Create Test Grid","text":"n_test_side = 100\nx_test = range(0.0, 1.0, length=n_test_side)\nX_test = Matrix{Float64}(undef, n_test_side^2, 2)\n\nidx = 1\nfor j in 1:n_test_side\n    for i in 1:n_test_side\n        X_test[idx, 1] = x_test[i]\n        X_test[idx, 2] = x_test[j]\n        idx += 1\n    end\nend\n\nprintln(\"Test grid: $(n_test_side^2) points\")","category":"section"},{"location":"examples/demo/#Full-GP-Model","page":"Local Approximate GP Demo","title":"Full GP Model","text":"First, build a full GP for comparison:\n\n# Estimate hyperparameters\nd_range = darg(X_train)\ng_range = garg(Z_train)\n\nprintln(\"Lengthscale range: $(d_range.min) to $(d_range.max)\")\nprintln(\"Nugget range: $(g_range.min) to $(g_range.max)\")\n\n# Create and fit GP\ngp = new_gp(X_train, Z_train, d_range.start, g_range.start)\njmle_gp(gp; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Optimized: d=$(gp.d), g=$(gp.g)\")\nprintln(\"Log-likelihood: $(llik_gp(gp))\")\n\n# Predict on test grid\npred_full = pred_gp(gp, X_test; lite=true)\n\nThe full GP provides smooth predictions over the entire domain:\n\n(Image: GP Mean Surface)\n\nThe variance surface shows uncertainty, which is lowest near training data:\n\n(Image: GP Variance Surface)","category":"section"},{"location":"examples/demo/#Local-Approximate-GP","page":"Local Approximate GP Demo","title":"Local Approximate GP","text":"Now use aGP with different acquisition methods:\n\n# ALC method (best accuracy)\nresult_alc = agp(X_train, Z_train, X_test;\n    start=6, endpt=50,\n    d=(start=gp.d, mle=false),\n    g=(start=gp.g, mle=false),\n    method=:alc\n)\n\n# NN method (fastest)\nresult_nn = agp(X_train, Z_train, X_test;\n    start=6, endpt=50,\n    d=(start=gp.d, mle=false),\n    g=(start=gp.g, mle=false),\n    method=:nn\n)\n\nThe aGP predictions are nearly indistinguishable from the full GP:\n\n(Image: aGP Predictions)","category":"section"},{"location":"examples/demo/#Compare-Results","page":"Local Approximate GP Demo","title":"Compare Results","text":"# True values for comparison\ntrue_vals = f2d(4.0 .* (X_test .- 0.5))\n\n# RMSE comparison\nrmse_full = sqrt(mean((pred_full.mean .- true_vals).^2))\nrmse_alc = sqrt(mean((result_alc.mean .- true_vals).^2))\nrmse_nn = sqrt(mean((result_nn.mean .- true_vals).^2))\n\nprintln(\"RMSE Comparison:\")\nprintln(\"  Full GP: $(round(rmse_full, digits=6))\")\nprintln(\"  aGP ALC: $(round(rmse_alc, digits=6))\")\nprintln(\"  aGP NN:  $(round(rmse_nn, digits=6))\")\n\nSide-by-side comparison of full GP vs local approximate GP predictions:\n\n(Image: Full GP vs aGP Comparison)","category":"section"},{"location":"examples/demo/#Local-Design-Visualization","page":"Local Approximate GP Demo","title":"Local Design Visualization","text":"Examine which points are selected for a specific prediction location:\n\n# Single prediction at center\nXref = [0.5, 0.5]\nlagp_result = lagp(Xref, 6, 30, X_train, Z_train;\n    d=gp.d, g=gp.g, method=:alc\n)\n\nprintln(\"Local design size: $(length(lagp_result.indices))\")\nprintln(\"Prediction: mean=$(lagp_result.mean), var=$(lagp_result.var)\")\n\nThe local design shows which training points were selected for predicting at the center:\n\n(Image: Local Design Selection)\n\nThe test function itself has interesting structure with constraints:\n\n(Image: Constrained Problem)","category":"section"},{"location":"examples/demo/#Key-Observations","page":"Local Approximate GP Demo","title":"Key Observations","text":"Full GP vs Local: For smooth functions, full GP and aGP produce similar predictions\nALC vs NN: ALC typically provides better accuracy by selecting informative points\nComputation: aGP scales better for large datasets (local designs are small)\nHyperparameters: Using MLE-optimized parameters from a full GP subset works well","category":"section"},{"location":"examples/demo/#Next-Steps","page":"Local Approximate GP Demo","title":"Next Steps","text":"Try different start and endpt values\nEnable local MLE with d=(start=..., mle=true)\nUse :mspe method for a balance of speed and accuracy","category":"section"},{"location":"#laGP.jl","page":"Home","title":"laGP.jl","text":"Local Approximate Gaussian Process Regression for Julia","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"laGP.jl is a Julia implementation of Local Approximate Gaussian Process (laGP) regression, ported from the R laGP package by Robert Gramacy. It enables scalable GP predictions for large datasets by building local GP models at each prediction point.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Scalable Predictions: Handle datasets with thousands of observations by building local GP models\nAbstractGPs.jl Backend: Built on the JuliaGaussianProcesses ecosystem for robust GP computations\nIsotropic & Separable Kernels: Single lengthscale or per-dimension ARD lengthscales\nAcquisition Functions: ALC (Active Learning Cohn) and MSPE for intelligent point selection\nMLE with Priors: Maximum likelihood estimation with Inverse-Gamma priors (MAP)\nMulti-threaded: Parallel predictions across test points","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/joshualeond/laGP.jl\")","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using laGP\nusing Random\n\nRandom.seed!(42)\n\n# Generate training data\nn = 100\nX = rand(n, 2)\nZ = sin.(2π * X[:, 1]) .* cos.(2π * X[:, 2]) + 0.1 * randn(n)\n\n# Estimate hyperparameters from data\nd_range = darg(X)\ng_range = garg(Z)\n\n# Create GP with initial parameters\ngp = new_gp(X, Z, d_range.start, g_range.start)\n\n# Optimize hyperparameters via joint MLE\njmle_gp(gp; drange=(d_range.min, d_range.max), grange=(g_range.min, g_range.max))\n\nprintln(\"Optimized lengthscale: \", gp.d)\nprintln(\"Optimized nugget: \", gp.g)\n\n# Make predictions\nX_test = rand(10, 2)\npred = pred_gp(gp, X_test)\n\nprintln(\"Predictions: \", pred.mean)\nprintln(\"Variances: \", pred.s2)","category":"section"},{"location":"#Design-Matrix-Convention","page":"Home","title":"Design Matrix Convention","text":"All design matrices use rows as observations:\n\nX is n × m where n is number of points and m is dimensionality\nThis matches the R laGP convention","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\n    \"getting_started.md\",\n    \"theory.md\",\n    \"examples/demo.md\",\n    \"examples/sinusoidal.md\",\n    \"examples/surrogates.md\",\n    \"examples/satellite.md\",\n    \"api.md\",\n]\nDepth = 2","category":"section"},{"location":"#References","page":"Home","title":"References","text":"Gramacy, R. B. (2016). laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R. Journal of Statistical Software, 72(1), 1-46.\nGramacy, R. B. (2020). Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.","category":"section"},{"location":"examples/satellite/#Satellite-Drag-Modeling-Example","page":"Satellite Drag Modeling","title":"Satellite Drag Modeling Example","text":"This example demonstrates GP surrogate modeling for GRACE satellite drag coefficients, based on Chapter 2 of \"Surrogates\" by Robert Gramacy.","category":"section"},{"location":"examples/satellite/#Overview","page":"Satellite Drag Modeling","title":"Overview","text":"We will:\n\nLoad GRACE satellite drag coefficient data\nFit separable GPs for multiple atmospheric species\nMake predictions and compute RMSPE\nCombine species predictions for atmospheric mixture","category":"section"},{"location":"examples/satellite/#Background","page":"Satellite Drag Modeling","title":"Background","text":"The GRACE satellite mission requires accurate drag coefficient (Cd) predictions for orbit determination. The drag coefficient depends on:\n\nVelocity magnitude (Umag)\nSurface temperature (Ts)\nAtmospheric temperature (Ta)\nYaw and pitch angles (theta, phi)\nAccommodation coefficients (alphan, sigmat)\n\nData is available for 6 atmospheric species: He, O, O2, N, N2, H.","category":"section"},{"location":"examples/satellite/#Setup","page":"Satellite Drag Modeling","title":"Setup","text":"using laGP\nusing Downloads\nusing DelimitedFiles\nusing Random\nusing Statistics: mean\n\nRandom.seed!(42)\n\n# Variable names\nvar_names = [\"Umag\", \"Ts\", \"Ta\", \"theta\", \"phi\", \"alphan\", \"sigmat\"]\n\n# Atmospheric species\nspecies_list = [:He, :O, :O2, :N, :N2, :H]\n\n# Molecular masses (g/mol)\nmolecular_mass = Dict(\n    :He => 4.003, :O => 15.999, :O2 => 31.998,\n    :N => 14.007, :N2 => 28.014, :H => 1.008\n)","category":"section"},{"location":"examples/satellite/#Load-Data","page":"Satellite Drag Modeling","title":"Load Data","text":"Download data from the TPM repository:\n\nfunction load_grace_data(species::Symbol, n::Int)\n    url = \"https://bitbucket.org/gramacylab/tpm/raw/master/data/GRACE/CD_GRACE_$(n)_$(species).csv\"\n    io = IOBuffer()\n    Downloads.download(url, io)\n    seekstart(io)\n    content = String(take!(io))\n\n    lines = split(content, '\\n')\n    data_rows = Float64[]\n    for line in lines[2:end]\n        if !isempty(strip(line))\n            values = parse.(Float64, split(line, ','))\n            append!(data_rows, values)\n        end\n    end\n\n    n_cols = length(split(lines[1], ','))\n    return reshape(data_rows, n_cols, :)'\nend\n\n# Load training (n=1000) and test (n=100) data\ntrain_data = Dict(s => load_grace_data(s, 1000) for s in species_list)\ntest_data = Dict(s => load_grace_data(s, 100) for s in species_list)\n\nprintln(\"Loaded data for $(length(species_list)) species\")\nprintln(\"Training: 1000 points, Test: 100 points\")","category":"section"},{"location":"examples/satellite/#Normalize-Data","page":"Satellite Drag Modeling","title":"Normalize Data","text":"function normalize_data(train, test)\n    X_train_raw = train[:, 1:7]\n    X_test_raw = test[:, 1:7]\n    Y_train = train[:, 8]\n    Y_test = test[:, 8]\n\n    # Global normalization\n    ranges = [(minimum(vcat(X_train_raw[:,j], X_test_raw[:,j])),\n               maximum(vcat(X_train_raw[:,j], X_test_raw[:,j]))) for j in 1:7]\n\n    X_train = similar(X_train_raw)\n    X_test = similar(X_test_raw)\n    for j in 1:7\n        X_train[:, j] = (X_train_raw[:, j] .- ranges[j][1]) ./ (ranges[j][2] - ranges[j][1])\n        X_test[:, j] = (X_test_raw[:, j] .- ranges[j][1]) ./ (ranges[j][2] - ranges[j][1])\n    end\n\n    return X_train, X_test, Y_train, Y_test\nend\n\nnormalized = Dict(s => normalize_data(train_data[s], test_data[s]) for s in species_list)","category":"section"},{"location":"examples/satellite/#Fit-Separable-GPs","page":"Satellite Drag Modeling","title":"Fit Separable GPs","text":"gp_models = Dict{Symbol, GPsep{Float64}}()\npredictions = Dict{Symbol, NamedTuple}()\nrmspe_values = Dict{Symbol, Float64}()\n\nfor species in species_list\n    X_train, X_test, Y_train, Y_test = normalized[species]\n\n    # Get hyperparameter ranges\n    d_range = darg_sep(X_train)\n    g_range = garg(Y_train)\n\n    d_start = [r.start for r in d_range.ranges]\n    d_ranges = [(r.min, r.max) for r in d_range.ranges]\n\n    # Fit GP\n    gp = new_gp_sep(X_train, Y_train, d_start, g_range.start)\n    jmle_gp_sep(gp; drange=d_ranges, grange=(g_range.min, g_range.max))\n\n    # Predict\n    pred = pred_gp_sep(gp, X_test; lite=true)\n\n    # RMSPE\n    pct_errors = ((pred.mean .- Y_test) ./ Y_test) .* 100\n    rmspe = sqrt(mean(pct_errors.^2))\n\n    gp_models[species] = gp\n    predictions[species] = (mean=pred.mean, Y_test=Y_test)\n    rmspe_values[species] = rmspe\n\n    println(\"$species RMSPE: $(round(rmspe, digits=3))%\")\nend\n\nRMSPE comparison across species:\n\n(Image: RMSPE by Species)\n\nParity plot for Helium predictions:\n\n(Image: Helium Parity Plot)","category":"section"},{"location":"examples/satellite/#Atmospheric-Mixture","page":"Satellite Drag Modeling","title":"Atmospheric Mixture","text":"Combine species predictions using mole fraction weighting:\n\n# Example mole fractions at ~400 km altitude\nmole_fractions = Dict(\n    :O => 0.70, :N2 => 0.15, :He => 0.08,\n    :O2 => 0.04, :N => 0.02, :H => 0.01\n)\n\n# Mass-weighted mixture\nnumerator = zeros(100)\ndenominator = zeros(100)\ntrue_numerator = zeros(100)\n\nfor species in species_list\n    weight = mole_fractions[species] * molecular_mass[species]\n    numerator .+= predictions[species].mean .* weight\n    true_numerator .+= predictions[species].Y_test .* weight\n    denominator .+= weight\nend\n\nCd_mix_pred = numerator ./ denominator\nCd_mix_true = true_numerator ./ denominator\n\n# Mixture RMSPE\npct_errors_mix = ((Cd_mix_pred .- Cd_mix_true) ./ Cd_mix_true) .* 100\nrmspe_mixture = sqrt(mean(pct_errors_mix.^2))\n\nprintln(\"Mixture RMSPE: $(round(rmspe_mixture, digits=3))%\")\n\nParity plot for the atmospheric mixture predictions:\n\n(Image: Mixture Parity Plot)","category":"section"},{"location":"examples/satellite/#Lengthscale-Analysis","page":"Satellite Drag Modeling","title":"Lengthscale Analysis","text":"println(\"\\nLengthscales by variable (smaller = more important):\")\nprintln(rpad(\"Species\", 8), join([rpad(v, 10) for v in var_names]))\n\nfor species in species_list\n    d = gp_models[species].d\n    vals = join([rpad(round(d[j], sigdigits=3), 10) for j in 1:7])\n    println(rpad(string(species), 8), vals)\nend\n\n# Average importance\navg_d = zeros(7)\nfor species in species_list\n    avg_d .+= gp_models[species].d\nend\navg_d ./= length(species_list)\n\nsorted_idx = sortperm(avg_d)\nprintln(\"\\nMost influential inputs:\")\nfor i in 1:3\n    j = sorted_idx[i]\n    println(\"  $(var_names[j]): avg lengthscale = $(round(avg_d[j], sigdigits=3))\")\nend\n\nLengthscales by species reveal which inputs matter most:\n\n(Image: Lengthscales by Species)","category":"section"},{"location":"examples/satellite/#Main-Effects-(for-Helium)","page":"Satellite Drag Modeling","title":"Main Effects (for Helium)","text":"gp_he = gp_models[:He]\nbaseline = fill(0.5, 7)\n\nn_me = 100\nx_me = range(0.0, 1.0, length=n_me)\n\nmain_effects_he = Matrix{Float64}(undef, n_me, 7)\n\nfor j in 1:7\n    XX = repeat(baseline', n_me, 1)\n    XX[:, j] = collect(x_me)\n    pred = pred_gp_sep(gp_he, XX; lite=true)\n    main_effects_he[:, j] = pred.mean\nend\n\nprintln(\"\\nHe sensitivity (effect range):\")\nfor j in 1:7\n    range_j = maximum(main_effects_he[:, j]) - minimum(main_effects_he[:, j])\n    println(\"  $(var_names[j]): $(round(range_j, digits=4))\")\nend\n\nMain effects for Helium show variable sensitivities:\n\n(Image: Main Effects for He)","category":"section"},{"location":"examples/satellite/#Visualization-(with-CairoMakie)","page":"Satellite Drag Modeling","title":"Visualization (with CairoMakie)","text":"using CairoMakie\n\n# Parity plot for Helium\nfig = Figure(size=(500, 450))\nax = Axis(fig[1, 1],\n    xlabel=\"True Cd\",\n    ylabel=\"Predicted Cd\",\n    title=\"He: Predicted vs True (RMSPE=$(round(rmspe_values[:He], digits=2))%)\",\n    aspect=DataAspect()\n)\n\nY_test = predictions[:He].Y_test\npred_he = predictions[:He].mean\n\nlims = (minimum(vcat(Y_test, pred_he)) - 0.1, maximum(vcat(Y_test, pred_he)) + 0.1)\nlines!(ax, [lims[1], lims[2]], [lims[1], lims[2]], color=:red, linewidth=2, linestyle=:dash)\nscatter!(ax, Y_test, pred_he, color=:blue, markersize=8, alpha=0.7)\n\nxlims!(ax, lims...)\nylims!(ax, lims...)\n\nfig","category":"section"},{"location":"examples/satellite/#Key-Findings","page":"Satellite Drag Modeling","title":"Key Findings","text":"Species variation: Different species have different drag characteristics\nInput importance: Velocity magnitude and accommodation coefficients are most influential\nMixture benefit: Mixture RMSPE is often lower due to averaging effects\nSeparable advantage: Per-dimension lengthscales reveal input sensitivities\nScalability: GPs trained on 1000 points predict accurately on held-out data","category":"section"},{"location":"examples/sinusoidal/#Posterior-Sampling-Example","page":"Posterior Sampling","title":"Posterior Sampling Example","text":"This example demonstrates GP posterior sampling using the full covariance matrix, fitting a sparse sinusoidal dataset.","category":"section"},{"location":"examples/sinusoidal/#Overview","page":"Posterior Sampling","title":"Overview","text":"We will:\n\nFit an isotropic GP to sparse sinusoidal data\nUse pred_gp(lite=false) to get the full posterior covariance\nDraw posterior samples from the GP\nVisualize the posterior uncertainty","category":"section"},{"location":"examples/sinusoidal/#Setup","page":"Posterior Sampling","title":"Setup","text":"using laGP\nusing Distributions\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(42)","category":"section"},{"location":"examples/sinusoidal/#Generate-Training-Data","page":"Posterior Sampling","title":"Generate Training Data","text":"Create sparse observations from sin(x):\n\n# Sparse training points over [0, 2π]\nX_train = reshape([0.5, 1.5, 2.5, 3.5, 5.0, 6.0], :, 1)\nY_train = sin.(X_train[:, 1])\n\nprintln(\"Training data:\")\nprintln(\"  X: \", vec(X_train))\nprintln(\"  Y: \", Y_train)","category":"section"},{"location":"examples/sinusoidal/#Fit-Isotropic-GP","page":"Posterior Sampling","title":"Fit Isotropic GP","text":"# Initial hyperparameters\nd_init = 2.0   # lengthscale\ng_init = 1e-6  # small nugget for near-interpolation\n\n# Create GP\ngp = new_gp(X_train, Y_train, d_init, g_init)\n\n# MLE for lengthscale only (keep small nugget fixed)\nmle_gp(gp, :d; tmax=20.0)\n\nprintln(\"Optimized hyperparameters:\")\nprintln(\"  d = \", gp.d)\nprintln(\"  g = \", gp.g)\nprintln(\"  log-likelihood = \", llik_gp(gp))","category":"section"},{"location":"examples/sinusoidal/#Full-Posterior-Prediction","page":"Posterior Sampling","title":"Full Posterior Prediction","text":"Get the full covariance matrix for sampling:\n\n# Dense test grid\nxx = collect(range(-0.5, 2π + 0.5, length=200))\nXX = reshape(xx, :, 1)\n\n# Get full posterior (lite=false returns GPPredictionFull)\npred_full = pred_gp(gp, XX; lite=false)\n\nprintln(\"Prediction:\")\nprintln(\"  Test points: \", length(xx))\nprintln(\"  Sigma size: \", size(pred_full.Sigma))","category":"section"},{"location":"examples/sinusoidal/#Draw-Posterior-Samples","page":"Posterior Sampling","title":"Draw Posterior Samples","text":"# Create multivariate normal distribution\nmvn = MvNormal(pred_full.mean, Symmetric(pred_full.Sigma))\n\n# Draw samples\nn_samples = 50\nsamples = rand(mvn, n_samples)\n\nprintln(\"Posterior samples: \", size(samples))","category":"section"},{"location":"examples/sinusoidal/#Visualization-(with-CairoMakie)","page":"Posterior Sampling","title":"Visualization (with CairoMakie)","text":"using CairoMakie\n\nfig = Figure(size=(700, 500))\nax = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"Y(x) | θ̂\",\n    title=\"GP Posterior Samples\"\n)\n\n# Draw posterior samples (gray, semi-transparent)\nfor i in 1:n_samples\n    lines!(ax, xx, samples[:, i], color=(:gray, 0.3), linewidth=0.5)\nend\n\n# Draw posterior mean (blue)\nlines!(ax, xx, pred_full.mean, color=:blue, linewidth=2, label=\"Posterior mean\")\n\n# Draw true function (dashed green)\nlines!(ax, xx, sin.(xx), color=:green, linewidth=1.5, linestyle=:dash, label=\"sin(x)\")\n\n# Draw training points (black circles)\nscatter!(ax, vec(X_train), Y_train, color=:black, markersize=12, label=\"Training data\")\n\naxislegend(ax, position=:lb)\nxlims!(ax, -0.5, 2π + 0.5)\nylims!(ax, -2.0, 2.0)\n\nfig\n\nThe resulting visualization shows the posterior samples with the GP mean and true function:\n\n(Image: GP Posterior Samples)","category":"section"},{"location":"examples/sinusoidal/#Key-Concepts","page":"Posterior Sampling","title":"Key Concepts","text":"","category":"section"},{"location":"examples/sinusoidal/#Posterior-Mean-and-Variance","page":"Posterior Sampling","title":"Posterior Mean and Variance","text":"The posterior mean interpolates the training data (due to small nugget), while the variance:\n\nIs near zero at training points\nIncreases between and beyond training points\nReflects uncertainty about the true function","category":"section"},{"location":"examples/sinusoidal/#Posterior-Samples","page":"Posterior Sampling","title":"Posterior Samples","text":"Each sample represents a plausible function consistent with:\n\nThe training data\nThe GP prior (smoothness encoded by kernel)\nThe estimated hyperparameters","category":"section"},{"location":"examples/sinusoidal/#Practical-Notes","page":"Posterior Sampling","title":"Practical Notes","text":"Nugget size: A very small nugget (1e-6) gives near-interpolation. Increase for noisy data.\nMemory: Full covariance requires O(n²) storage. For large test sets, use lite=true.\nNumerical stability: The covariance matrix may need symmetrization before sampling.","category":"section"},{"location":"examples/sinusoidal/#Credible-Intervals","page":"Posterior Sampling","title":"Credible Intervals","text":"Instead of sampling, you can compute pointwise intervals:\n\nusing Distributions: TDist, quantile\n\n# 95% credible intervals using Student-t distribution\nt_dist = TDist(pred_full.df)\nt_crit = quantile(t_dist, 0.975)\n\n# Standard deviation at each point\nstd_pred = sqrt.(diag(pred_full.Sigma))\n\n# Intervals\nlower = pred_full.mean .- t_crit .* std_pred\nupper = pred_full.mean .+ t_crit .* std_pred\n\nThis is more efficient than sampling when you only need intervals.","category":"section"}]
}
